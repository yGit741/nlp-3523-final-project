{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Knowledge vs Reasoning Separation: Proof of Concept\n",
        "\n",
        "This notebook implements a proof of concept for separating knowledge and reasoning in language models through ε-masking and structural hints.\n",
        "\n",
        "## Research Question\n",
        "How does limiting contextual knowledge with ε-masking and structural hints affect performance across tasks that differ in their demands for knowledge and reasoning?\n",
        "\n",
        "## Method Overview\n",
        "- Train small models from scratch with controlled knowledge exposure\n",
        "- Apply ε-masking (ε ∈ {0.05–0.50}) while preserving structural hints\n",
        "- Compare performance across 4 task quadrants\n",
        "- Focus on trends rather than absolute performance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Installation and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (2.2.2)\n",
            "Requirement already satisfied: datasets in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (2.19.1)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.8.7-cp311-cp311-macosx_11_0_arm64.whl.metadata (27 kB)\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: scikit-learn in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (1.4.1.post1)\n",
            "Requirement already satisfied: matplotlib in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (3.8.2)\n",
            "Requirement already satisfied: seaborn in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (0.13.2)\n",
            "Requirement already satisfied: pandas in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (2.2.1)\n",
            "Requirement already satisfied: numpy in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (1.26.3)\n",
            "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from transformers) (2024.7.24)\n",
            "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from torch) (2024.3.1)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from datasets) (3.11.11)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
            "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
            "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
            "  Downloading murmurhash-1.0.13-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.2 kB)\n",
            "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
            "  Downloading cymem-2.0.11-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.5 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
            "  Downloading preshed-3.0.10-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
            "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
            "  Downloading thinc-8.3.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (15 kB)\n",
            "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
            "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
            "  Downloading srsly-2.5.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (19 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
            "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
            "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
            "  Downloading typer-0.17.4-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from spacy) (2.10.4)\n",
            "Requirement already satisfied: setuptools in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from spacy) (68.2.2)\n",
            "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
            "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: click in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from scikit-learn) (1.12.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from scikit-learn) (3.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from matplotlib) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: pillow>=8 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from matplotlib) (10.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from aiohttp->datasets) (1.18.3)\n",
            "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
            "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from requests->transformers) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
            "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Downloading blis-1.3.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
            "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
            "INFO: pip is looking at multiple versions of thinc to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
            "  Downloading thinc-8.3.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (15 kB)\n",
            "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Downloading blis-1.2.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
            "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Downloading cloudpathlib-0.22.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Downloading smart_open-7.3.0.post1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
            "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
            "  Downloading marisa_trie-1.3.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (10 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.17.2)\n",
            "Requirement already satisfied: wrapt in /opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
            "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Downloading spacy-3.8.7-cp311-cp311-macosx_11_0_arm64.whl (6.4 MB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
            "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
            "Downloading cymem-2.0.11-cp311-cp311-macosx_11_0_arm64.whl (41 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.0/183.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading murmurhash-1.0.13-cp311-cp311-macosx_11_0_arm64.whl (26 kB)\n",
            "Downloading preshed-3.0.10-cp311-cp311-macosx_11_0_arm64.whl (127 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
            "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
            "Downloading srsly-2.5.1-cp311-cp311-macosx_11_0_arm64.whl (634 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m634.4/634.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.3.4-cp311-cp311-macosx_11_0_arm64.whl (774 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.2/774.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.17.4-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.6/46.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
            "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blis-1.2.1-cp311-cp311-macosx_11_0_arm64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
            "\u001b[?25hDownloading cloudpathlib-0.22.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
            "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
            "\u001b[?25hDownloading rich-14.1.0-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Downloading smart_open-7.3.0.post1-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marisa_trie-1.3.1-cp311-cp311-macosx_11_0_arm64.whl (158 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.5/158.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.3/87.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, smart-open, shellingham, nltk, murmurhash, mdurl, marisa-trie, cloudpathlib, catalogue, blis, srsly, preshed, markdown-it-py, language-data, rich, langcodes, confection, typer, thinc, weasel, spacy\n",
            "Successfully installed blis-1.2.1 catalogue-2.0.10 cloudpathlib-0.22.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.3.1 markdown-it-py-4.0.0 mdurl-0.1.2 murmurhash-1.0.13 nltk-3.9.1 preshed-3.0.10 rich-14.1.0 shellingham-1.5.4 smart-open-7.3.0.post1 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 typer-0.17.4 wasabi-1.1.3 weasel-0.4.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
            "Successfully installed en-core-web-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "%pip install transformers torch datasets spacy nltk scikit-learn matplotlib seaborn pandas numpy\n",
        "!python -m spacy download en_core_web_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All packages imported successfully!\n",
            "PyTorch version: 2.2.2\n",
            "Transformers version: 4.44.2\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import transformers\n",
        "from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer, AutoTokenizer, AutoModelForCausalLM\n",
        "import spacy\n",
        "import nltk\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from datasets import load_dataset\n",
        "import json\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "print(\"All packages imported successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n",
            "Configuration loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "class Config:\n",
        "    # Model parameters\n",
        "    MODEL_SIZE = \"gpt2\"  # Will use GPT-2 small for POC\n",
        "    VOCAB_SIZE = 50257\n",
        "    MAX_LENGTH = 512\n",
        "    BATCH_SIZE = 8\n",
        "    LEARNING_RATE = 5e-4\n",
        "    NUM_EPOCHS = 3\n",
        "    \n",
        "    # Masking parameters\n",
        "    EPSILON_VALUES = [0.05, 0.15, 0.30, 0.50]  # Different masking levels\n",
        "    \n",
        "    # Structural hints\n",
        "    PRESERVE_FUNCTION_WORDS = True\n",
        "    PRESERVE_PUNCTUATION = True\n",
        "    PRESERVE_NER = True\n",
        "    \n",
        "    # Device\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "    \n",
        "    # Random seed\n",
        "    SEED = 42\n",
        "\n",
        "# Set random seeds\n",
        "torch.manual_seed(Config.SEED)\n",
        "np.random.seed(Config.SEED)\n",
        "random.seed(Config.SEED)\n",
        "\n",
        "print(f\"Using device: {Config.DEVICE}\")\n",
        "print(f\"Configuration loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Preprocessing for Structural Hints\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load spaCy model for NER\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"spaCy model loaded successfully!\")\n",
        "except OSError:\n",
        "    print(\"Please install spaCy English model: python -m spacy download en_core_web_sm\")\n",
        "    nlp = None\n",
        "\n",
        "# Function words (from NLTK)\n",
        "from nltk.corpus import stopwords\n",
        "function_words = set(stopwords.words('english'))\n",
        "\n",
        "# Punctuation patterns\n",
        "punctuation_pattern = re.compile(r'[\\\\.,!?;:\\\"\\\\-\\\\[\\\\](){}]')\n",
        "\n",
        "class StructuralHintProcessor:\n",
        "    \"\"\"\n",
        "    Processes text to extract and preserve structural hints while preparing for masking.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, preserve_function_words=True, preserve_punctuation=True, preserve_ner=True):\n",
        "        self.preserve_function_words = preserve_function_words\n",
        "        self.preserve_punctuation = preserve_punctuation\n",
        "        self.preserve_ner = preserve_ner\n",
        "        self.function_words = function_words if preserve_function_words else set()\n",
        "        \n",
        "    def extract_structural_hints(self, text: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Extract structural hints from text.\n",
        "        Returns: dict with function_words, punctuation, ner_entities\n",
        "        \"\"\"\n",
        "        hints = {\n",
        "            'function_words': set(),\n",
        "            'punctuation': set(),\n",
        "            'ner_entities': {},\n",
        "            'tokens': text.split()\n",
        "        }\n",
        "        \n",
        "        # Extract function words\n",
        "        if self.preserve_function_words:\n",
        "            for token in hints['tokens']:\n",
        "                clean_token = re.sub(r'[^a-zA-Z]', '', token.lower())\n",
        "                if clean_token in self.function_words:\n",
        "                    hints['function_words'].add(token)\n",
        "        \n",
        "        # Extract punctuation\n",
        "        if self.preserve_punctuation:\n",
        "            for match in punctuation_pattern.finditer(text):\n",
        "                hints['punctuation'].add(match.group())\n",
        "        \n",
        "        # Extract NER entities\n",
        "        if self.preserve_ner and nlp:\n",
        "            doc = nlp(text)\n",
        "            entity_id = 0\n",
        "            for ent in doc.ents:\n",
        "                entity_type = ent.label_\n",
        "                entity_text = ent.text\n",
        "                hints['ner_entities'][entity_text] = f\"<{entity_type}_{entity_id}>\"\n",
        "                entity_id += 1\n",
        "        \n",
        "        return hints\n",
        "    \n",
        "    def apply_epsilon_masking(self, text: str, epsilon: float) -> str:\n",
        "        \"\"\"\n",
        "        Apply ε-masking to text while preserving structural hints.\n",
        "        \"\"\"\n",
        "        hints = self.extract_structural_hints(text)\n",
        "        tokens = hints['tokens']\n",
        "        masked_tokens = []\n",
        "        \n",
        "        for token in tokens:\n",
        "            # Check if token should be preserved\n",
        "            should_preserve = False\n",
        "            \n",
        "            # Preserve function words\n",
        "            if self.preserve_function_words:\n",
        "                clean_token = re.sub(r'[^a-zA-Z]', '', token.lower())\n",
        "                if clean_token in self.function_words:\n",
        "                    should_preserve = True\n",
        "            \n",
        "            # Preserve punctuation\n",
        "            if self.preserve_punctuation and punctuation_pattern.match(token):\n",
        "                should_preserve = True\n",
        "            \n",
        "            # Preserve NER entities (replace with typed IDs)\n",
        "            if self.preserve_ner and token in hints['ner_entities']:\n",
        "                masked_tokens.append(hints['ner_entities'][token])\n",
        "                should_preserve = True\n",
        "            \n",
        "            # Apply masking based on epsilon\n",
        "            if not should_preserve:\n",
        "                if random.random() < epsilon:\n",
        "                    masked_tokens.append('<MASK>')\n",
        "                else:\n",
        "                    masked_tokens.append(token)\n",
        "            else:\n",
        "                masked_tokens.append(token)\n",
        "        \n",
        "        return ' '.join(masked_tokens)\n",
        "\n",
        "# Initialize processor\n",
        "processor = StructuralHintProcessor(\n",
        "    preserve_function_words=Config.PRESERVE_FUNCTION_WORDS,\n",
        "    preserve_punctuation=Config.PRESERVE_PUNCTUATION,\n",
        "    preserve_ner=Config.PRESERVE_NER\n",
        ")\n",
        "\n",
        "print(\"Structural hint processor initialized!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Tokenization and Dimensionality Strategy\n",
        "\n",
        "**Key Challenges:**\n",
        "1. **Vocabulary Expansion**: NER entities become typed IDs (e.g., `<PERSON_0>`, `<ORG_1>`)\n",
        "2. **Dynamic Vocabulary**: Each document may introduce new entity IDs\n",
        "3. **Consistent Mapping**: Same entity should get same ID across contexts\n",
        "4. **Mask Token**: Need special `<MASK>` token in vocabulary\n",
        "\n",
        "**Solution Approach:**\n",
        "- Use a **hybrid tokenization** strategy\n",
        "- **Base vocabulary**: Standard GPT-2 tokenizer (50,257 tokens)\n",
        "- **Extended vocabulary**: Add special tokens for masking and entity types\n",
        "- **Entity mapping**: Maintain consistent entity-to-ID mapping per document\n",
        "- **Fixed sequence length**: Pad/truncate to consistent length\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EnhancedTokenizer:\n",
        "    \"\"\"\n",
        "    Enhanced tokenizer that handles structural hints and masking.\n",
        "    Based on GPT-2 tokenizer with extensions for our research.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, base_model=\"gpt2\"):\n",
        "        # Load base tokenizer\n",
        "        self.base_tokenizer = GPT2Tokenizer.from_pretrained(base_model)\n",
        "        self.base_tokenizer.pad_token = self.base_tokenizer.eos_token\n",
        "        \n",
        "        # Add special tokens for our research\n",
        "        special_tokens = {\n",
        "            'mask_token': '<MASK>',\n",
        "            'entity_start': '<ENT>',\n",
        "            'entity_end': '</ENT>',\n",
        "            'person_token': '<PERSON>',\n",
        "            'org_token': '<ORG>',\n",
        "            'location_token': '<LOC>',\n",
        "            'date_token': '<DATE>',\n",
        "            'misc_token': '<MISC>'\n",
        "        }\n",
        "        \n",
        "        # Add special tokens to vocabulary\n",
        "        self.base_tokenizer.add_special_tokens(special_tokens)\n",
        "        \n",
        "        # Store token mappings\n",
        "        self.mask_token_id = self.base_tokenizer.convert_tokens_to_ids('<MASK>')\n",
        "        self.pad_token_id = self.base_tokenizer.convert_tokens_to_ids('<|endoftext|>')\n",
        "        \n",
        "        # Entity ID counter (per document)\n",
        "        self.entity_counter = 0\n",
        "        self.entity_mapping = {}\n",
        "        \n",
        "        print(f\"Enhanced tokenizer initialized with {len(self.base_tokenizer)} tokens\")\n",
        "        print(f\"Special tokens: {special_tokens}\")\n",
        "    \n",
        "    def reset_entity_mapping(self):\n",
        "        \"\"\"Reset entity mapping for new document.\"\"\"\n",
        "        self.entity_counter = 0\n",
        "        self.entity_mapping = {}\n",
        "    \n",
        "    def create_entity_token(self, entity_type: str) -> str:\n",
        "        \"\"\"Create a unique entity token for this document.\"\"\"\n",
        "        entity_token = f\"<{entity_type}_{self.entity_counter}>\"\n",
        "        self.entity_counter += 1\n",
        "        return entity_token\n",
        "    \n",
        "    def tokenize_with_structural_hints(self, text: str, epsilon: float = 0.0) -> Dict:\n",
        "        \"\"\"\n",
        "        Tokenize text with structural hints and masking.\n",
        "        Returns tokenized input with metadata.\n",
        "        \"\"\"\n",
        "        # Reset entity mapping for this text\n",
        "        self.reset_entity_mapping()\n",
        "        \n",
        "        # Apply structural hint processing and masking\n",
        "        processor = StructuralHintProcessor()\n",
        "        masked_text = processor.apply_epsilon_masking(text, epsilon)\n",
        "        \n",
        "        # Tokenize the masked text\n",
        "        tokens = self.base_tokenizer.encode(masked_text, add_special_tokens=True)\n",
        "        \n",
        "        # Create attention mask (1 for real tokens, 0 for padding)\n",
        "        attention_mask = [1] * len(tokens)\n",
        "        \n",
        "        # Pad or truncate to fixed length\n",
        "        max_length = 512\n",
        "        if len(tokens) > max_length:\n",
        "            tokens = tokens[:max_length]\n",
        "            attention_mask = attention_mask[:max_length]\n",
        "        else:\n",
        "            # Pad with pad token\n",
        "            padding_length = max_length - len(tokens)\n",
        "            tokens.extend([self.pad_token_id] * padding_length)\n",
        "            attention_mask.extend([0] * padding_length)\n",
        "        \n",
        "        return {\n",
        "            'input_ids': torch.tensor(tokens, dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
        "            'original_text': text,\n",
        "            'masked_text': masked_text,\n",
        "            'epsilon': epsilon\n",
        "        }\n",
        "    \n",
        "    def decode_tokens(self, token_ids: List[int]) -> str:\n",
        "        \"\"\"Decode token IDs back to text.\"\"\"\n",
        "        return self.base_tokenizer.decode(token_ids, skip_special_tokens=False)\n",
        "\n",
        "# Initialize enhanced tokenizer\n",
        "enhanced_tokenizer = EnhancedTokenizer()\n",
        "print(\"Enhanced tokenizer ready!\")\n",
        "\n",
        "# Demo the tokenization process\n",
        "demo_text = \"The Eiffel Tower in Paris was designed by Gustave Eiffel, a French engineer.\"\n",
        "print(f\"\\nDemo text: {demo_text}\")\n",
        "\n",
        "for epsilon in [0.1, 0.3, 0.5]:\n",
        "    result = enhanced_tokenizer.tokenize_with_structural_hints(demo_text, epsilon)\n",
        "    print(f\"\\nε={epsilon}:\")\n",
        "    print(f\"Masked text: {result['masked_text']}\")\n",
        "    print(f\"Token count: {len(result['input_ids'])}\")\n",
        "    print(f\"Input shape: {result['input_ids'].shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Andrej Karpathy-Style Nano GPT with Masking\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"\"\"LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False\"\"\"\n",
        "    \n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"Multi-head causal self-attention with optional masking awareness.\"\"\"\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
        "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                        .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        if self.flash:\n",
        "            # efficient attention using Flash Attention CUDA kernels\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
        "        else:\n",
        "            # manual implementation of attention\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"Simple MLP with GELU activation.\"\"\"\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu    = nn.GELU()\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"Transformer block: communication followed by computation.\"\"\"\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        x = x + self.attn(self.ln_1(x), attention_mask)\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "class GPTConfig:\n",
        "    \"\"\"Configuration for GPT model.\"\"\"\n",
        "    \n",
        "    def __init__(self, **kwargs):\n",
        "        # Model parameters\n",
        "        self.block_size = kwargs.get('block_size', 512)\n",
        "        self.vocab_size = kwargs.get('vocab_size', 50257)\n",
        "        self.n_layer = kwargs.get('n_layer', 6)\n",
        "        self.n_head = kwargs.get('n_head', 8)\n",
        "        self.n_embd = kwargs.get('n_embd', 256)\n",
        "        self.dropout = kwargs.get('dropout', 0.1)\n",
        "        self.bias = kwargs.get('bias', False)\n",
        "        \n",
        "        # Masking parameters\n",
        "        self.mask_aware = kwargs.get('mask_aware', True)\n",
        "        self.epsilon = kwargs.get('epsilon', 0.0)\n",
        "\n",
        "class NanoGPT(nn.Module):\n",
        "    \"\"\"\n",
        "    Andrej Karpathy-style Nano GPT with masking awareness.\n",
        "    Adapted for knowledge vs reasoning separation research.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        \n",
        "        # Mask-aware components\n",
        "        if config.mask_aware:\n",
        "            self.mask_embedding = nn.Embedding(2, config.n_embd)  # 0: normal, 1: masked\n",
        "            self.mask_classifier = nn.Linear(config.n_embd, 2)  # Predict if token is masked\n",
        "\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        print(f\"number of parameters: {n_params/1e6:.2f}M\")\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None, attention_mask=None, epsilon=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        \n",
        "        # Add mask-aware embeddings if enabled\n",
        "        if self.config.mask_aware and attention_mask is not None:\n",
        "            # Create mask indicators (1 for masked tokens, 0 for normal)\n",
        "            mask_indicators = (idx == enhanced_tokenizer.mask_token_id).long()\n",
        "            mask_emb = self.mask_embedding(mask_indicators)\n",
        "            x = x + mask_emb\n",
        "\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x, attention_mask)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def crop_block_size(self, block_size):\n",
        "        # model surgery to decrease the block size if necessary\n",
        "        # e.g. we may load the model pre-trained with a block size of 1024\n",
        "        # but want to use it with a smaller block size for some reason such as\n",
        "        # smaller memory usage.\n",
        "        assert block_size <= self.config.block_size\n",
        "        self.config.block_size = block_size\n",
        "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n",
        "        for block in self.transformer.h:\n",
        "            if hasattr(block.attn, 'bias'):\n",
        "                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        # start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # filter out those that do not require gradients\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "# Import required modules for the GPT implementation\n",
        "import math\n",
        "import inspect\n",
        "import torch.nn.functional as F\n",
        "\n",
        "print(\"NanoGPT implementation ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Training Procedure with ε-Masking\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MaskedDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for training with ε-masking.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, texts: List[str], tokenizer: EnhancedTokenizer, epsilon: float):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.epsilon = epsilon\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        \n",
        "        # Tokenize with structural hints and masking\n",
        "        result = self.tokenizer.tokenize_with_structural_hints(text, self.epsilon)\n",
        "        \n",
        "        # For language modeling, targets are the input shifted by 1\n",
        "        input_ids = result['input_ids']\n",
        "        targets = torch.cat([input_ids[1:], torch.tensor([self.tokenizer.pad_token_id])])\n",
        "        \n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'targets': targets,\n",
        "            'attention_mask': result['attention_mask'],\n",
        "            'original_text': result['original_text'],\n",
        "            'masked_text': result['masked_text'],\n",
        "            'epsilon': self.epsilon\n",
        "        }\n",
        "\n",
        "def train_model_with_epsilon_masking(texts: List[str], epsilon: float, num_epochs: int = 3) -> Tuple[NanoGPT, EnhancedTokenizer]:\n",
        "    \"\"\"\n",
        "    Train a NanoGPT model with specific epsilon masking level.\n",
        "    \"\"\"\n",
        "    print(f\"\\\\n=== Training model with ε={epsilon} ===\")\n",
        "    \n",
        "    # Create model configuration\n",
        "    config = GPTConfig(\n",
        "        block_size=512,\n",
        "        vocab_size=len(enhanced_tokenizer.base_tokenizer),\n",
        "        n_layer=6,\n",
        "        n_head=8,\n",
        "        n_embd=256,\n",
        "        dropout=0.1,\n",
        "        bias=False,\n",
        "        mask_aware=True,\n",
        "        epsilon=epsilon\n",
        "    )\n",
        "    \n",
        "    # Initialize model\n",
        "    model = NanoGPT(config)\n",
        "    model.to(Config.DEVICE)\n",
        "    \n",
        "    # Create dataset and dataloader\n",
        "    dataset = MaskedDataset(texts, enhanced_tokenizer, epsilon)\n",
        "    dataloader = DataLoader(dataset, batch_size=Config.BATCH_SIZE, shuffle=True)\n",
        "    \n",
        "    # Setup optimizer\n",
        "    optimizer = model.configure_optimizers(\n",
        "        weight_decay=0.1,\n",
        "        learning_rate=Config.LEARNING_RATE,\n",
        "        betas=(0.9, 0.95),\n",
        "        device_type='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    )\n",
        "    \n",
        "    # Training loop\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "        \n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(Config.DEVICE)\n",
        "            targets = batch['targets'].to(Config.DEVICE)\n",
        "            attention_mask = batch['attention_mask'].to(Config.DEVICE)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass\n",
        "            logits, loss = model(input_ids, targets=targets, attention_mask=attention_mask, epsilon=epsilon)\n",
        "            \n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "        \n",
        "        avg_loss = total_loss / num_batches\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "    \n",
        "    return model, enhanced_tokenizer\n",
        "\n",
        "def evaluate_model_on_tasks(model: NanoGPT, tokenizer: EnhancedTokenizer, tasks: List[Dict]) -> Dict:\n",
        "    \"\"\"\n",
        "    Evaluate model on benchmark tasks.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    results = []\n",
        "    \n",
        "    for task in tasks:\n",
        "        context = task['context']\n",
        "        question = task['question']\n",
        "        options = task['options']\n",
        "        correct_answer = task['answer']\n",
        "        \n",
        "        # Create input prompt\n",
        "        prompt = f\"{context} {question} Answer:\"\n",
        "        \n",
        "        # Get model predictions for each option\n",
        "        option_scores = []\n",
        "        for option in options:\n",
        "            full_prompt = f\"{prompt} {option}\"\n",
        "            result = tokenizer.tokenize_with_structural_hints(full_prompt, epsilon=0.0)\n",
        "            input_ids = result['input_ids'].unsqueeze(0).to(Config.DEVICE)\n",
        "            attention_mask = result['attention_mask'].unsqueeze(0).to(Config.DEVICE)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                logits, _ = model(input_ids, attention_mask=attention_mask)\n",
        "                # Use the last token's probability as score\n",
        "                last_token_logits = logits[0, -1, :]\n",
        "                option_score = torch.softmax(last_token_logits, dim=-1).max().item()\n",
        "                option_scores.append(option_score)\n",
        "        \n",
        "        # Select highest scoring option\n",
        "        predicted_idx = np.argmax(option_scores)\n",
        "        predicted_answer = options[predicted_idx]\n",
        "        is_correct = predicted_answer == correct_answer\n",
        "        \n",
        "        results.append({\n",
        "            'task_id': task['id'],\n",
        "            'predicted_answer': predicted_answer,\n",
        "            'correct_answer': correct_answer,\n",
        "            'is_correct': is_correct,\n",
        "            'confidence': max(option_scores),\n",
        "            'reasoning_type': task['reasoning_type']\n",
        "        })\n",
        "    \n",
        "    accuracy = sum(r['is_correct'] for r in results) / len(results)\n",
        "    avg_confidence = sum(r['confidence'] for r in results) / len(results)\n",
        "    \n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'avg_confidence': avg_confidence,\n",
        "        'total_tasks': len(results),\n",
        "        'correct_tasks': sum(r['is_correct'] for r in results),\n",
        "        'detailed_results': results\n",
        "    }\n",
        "\n",
        "print(\"Training and evaluation functions ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Run the Complete Experiment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample training data (in practice, you'd use a larger dataset like WikiText-103)\n",
        "sample_texts = [\n",
        "    \"The cat sat on the mat and purred contentedly.\",\n",
        "    \"Paris is the capital of France and a major European city.\",\n",
        "    \"All birds can fly, but penguins are flightless birds.\",\n",
        "    \"The Eiffel Tower was built in Paris by Gustave Eiffel.\",\n",
        "    \"If it rains, the ground gets wet from the water.\",\n",
        "    \"Shakespeare wrote many famous plays including Hamlet and Macbeth.\",\n",
        "    \"The sun rises in the east and sets in the west.\",\n",
        "    \"John has three apples and gives one to Mary.\",\n",
        "    \"The trophy doesn't fit in the brown suitcase because it is too large.\",\n",
        "    \"The city councilmen refused the demonstrators a permit because they feared violence.\",\n",
        "    \"All birds can fly. Penguins are birds. Can penguins fly?\",\n",
        "    \"If it rains, the ground gets wet. The ground is wet. Did it rain?\"\n",
        "]\n",
        "\n",
        "print(f\"Training with {len(sample_texts)} sample texts\")\n",
        "print(\"Sample texts:\")\n",
        "for i, text in enumerate(sample_texts[:3]):\n",
        "    print(f\"{i+1}. {text}\")\n",
        "\n",
        "# Run the complete experiment\n",
        "print(\"\\\\n\" + \"=\"*60)\n",
        "print(\"STARTING KNOWLEDGE VS REASONING SEPARATION EXPERIMENT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "results = {}\n",
        "\n",
        "for epsilon in Config.EPSILON_VALUES:\n",
        "    print(f\"\\\\n{'='*20} ε={epsilon} {'='*20}\")\n",
        "    \n",
        "    # Train model with this epsilon\n",
        "    model, tokenizer = train_model_with_epsilon_masking(sample_texts, epsilon, num_epochs=2)\n",
        "    \n",
        "    # Evaluate on all quadrants\n",
        "    epsilon_results = {}\n",
        "    \n",
        "    for quadrant_name, tasks in benchmark_tasks.items():\n",
        "        print(f\"\\\\nEvaluating {quadrant_name}...\")\n",
        "        quadrant_result = evaluate_model_on_tasks(model, tokenizer, tasks)\n",
        "        epsilon_results[quadrant_name] = quadrant_result\n",
        "        print(f\"Accuracy: {quadrant_result['accuracy']:.3f} ({quadrant_result['correct_tasks']}/{quadrant_result['total_tasks']})\")\n",
        "    \n",
        "    results[epsilon] = epsilon_results\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*60)\n",
        "print(\"EXPERIMENT COMPLETED!\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Results Analysis and Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_results(results: Dict):\n",
        "    \"\"\"\n",
        "    Create visualizations of the experimental results.\n",
        "    \"\"\"\n",
        "    # Prepare data for plotting\n",
        "    epsilon_values = list(results.keys())\n",
        "    quadrants = list(results[epsilon_values[0]].keys())\n",
        "    \n",
        "    # Create accuracy plot\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    \n",
        "    for quadrant in quadrants:\n",
        "        accuracies = [results[eps][quadrant]['accuracy'] for eps in epsilon_values]\n",
        "        plt.plot(epsilon_values, accuracies, marker='o', label=quadrant.replace('_', ' ').title(), linewidth=2)\n",
        "    \n",
        "    plt.xlabel('Epsilon (Masking Level)', fontsize=12)\n",
        "    plt.ylabel('Accuracy', fontsize=12)\n",
        "    plt.title('Performance vs. Knowledge Masking Level', fontsize=14, fontweight='bold')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Create confidence plot\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    \n",
        "    for quadrant in quadrants:\n",
        "        confidences = [results[eps][quadrant]['avg_confidence'] for eps in epsilon_values]\n",
        "        plt.plot(epsilon_values, confidences, marker='s', label=quadrant.replace('_', ' ').title(), linewidth=2)\n",
        "    \n",
        "    plt.xlabel('Epsilon (Masking Level)', fontsize=12)\n",
        "    plt.ylabel('Average Confidence', fontsize=12)\n",
        "    plt.title('Model Confidence vs. Knowledge Masking Level', fontsize=14, fontweight='bold')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def analyze_results(results: Dict):\n",
        "    \"\"\"\n",
        "    Analyze and print key findings from the results.\n",
        "    \"\"\"\n",
        "    print(\"\\\\n\" + \"=\"*60)\n",
        "    print(\"EXPERIMENTAL RESULTS ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    epsilon_values = list(results.keys())\n",
        "    quadrants = list(results[epsilon_values[0]].keys())\n",
        "    \n",
        "    for quadrant in quadrants:\n",
        "        print(f\"\\\\n{quadrant.replace('_', ' ').title()}:\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        accuracies = [results[eps][quadrant]['accuracy'] for eps in epsilon_values]\n",
        "        \n",
        "        for i, eps in enumerate(epsilon_values):\n",
        "            print(f\"ε={eps}: Accuracy={accuracies[i]:.3f}\")\n",
        "        \n",
        "        # Find best epsilon for this quadrant\n",
        "        best_eps_idx = np.argmax(accuracies)\n",
        "        best_eps = epsilon_values[best_eps_idx]\n",
        "        best_acc = accuracies[best_eps_idx]\n",
        "        \n",
        "        print(f\"Best performance: ε={best_eps} (Accuracy={best_acc:.3f})\")\n",
        "        \n",
        "        # Check if masking helps\n",
        "        baseline_acc = accuracies[0]  # ε=0.05 (minimal masking)\n",
        "        max_acc = max(accuracies)\n",
        "        \n",
        "        if max_acc > baseline_acc:\n",
        "            improvement = max_acc - baseline_acc\n",
        "            print(f\"✓ Masking helps! Improvement: +{improvement:.3f}\")\n",
        "        else:\n",
        "            print(\"✗ Masking does not help in this quadrant\")\n",
        "    \n",
        "    # Overall analysis\n",
        "    print(\"\\\\n\" + \"=\"*60)\n",
        "    print(\"KEY FINDINGS\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Check if light knowledge + heavy reasoning benefits from masking\n",
        "    lk_hr_quadrant = \"light_knowledge_heavy_reasoning\"\n",
        "    if lk_hr_quadrant in quadrants:\n",
        "        lk_hr_accs = [results[eps][lk_hr_quadrant]['accuracy'] for eps in epsilon_values]\n",
        "        baseline_lk_hr = lk_hr_accs[0]\n",
        "        best_lk_hr = max(lk_hr_accs)\n",
        "        \n",
        "        if best_lk_hr > baseline_lk_hr:\n",
        "            print(f\"✓ HYPOTHESIS SUPPORTED: Light Knowledge + Heavy Reasoning benefits from masking!\")\n",
        "            print(f\"  Improvement: {best_lk_hr - baseline_lk_hr:.3f}\")\n",
        "        else:\n",
        "            print(\"✗ HYPOTHESIS NOT SUPPORTED: Masking does not help Light Knowledge + Heavy Reasoning\")\n",
        "    \n",
        "    print(\"\\\\nExperiment completed successfully!\")\n",
        "\n",
        "# Visualize and analyze results\n",
        "if 'results' in locals() and results:\n",
        "    visualize_results(results)\n",
        "    analyze_results(results)\n",
        "else:\n",
        "    print(\"No results available yet. Please run the experiment first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Demo: Structural Hint Processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo the structural hint processing\n",
        "demo_text = \"The Eiffel Tower in Paris was designed by Gustave Eiffel, a French engineer.\"\n",
        "\n",
        "print(\"Original text:\")\n",
        "print(demo_text)\n",
        "print()\n",
        "\n",
        "for epsilon in [0.1, 0.3, 0.5]:\n",
        "    masked_text = processor.apply_epsilon_masking(demo_text, epsilon)\n",
        "    print(f\"ε={epsilon}: {masked_text}\")\n",
        "\n",
        "print(\"\\\\nNote: Function words, punctuation, and NER entities are preserved while other content is masked.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Next Steps and Extensions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\"\"\n",
        "NEXT STEPS FOR FULL RESEARCH:\n",
        "\n",
        "1. SCALE UP:\n",
        "   - Use larger datasets (WikiText-103, OSCAR)\n",
        "   - Train for more epochs with proper validation\n",
        "   - Use larger models (GPT-2 small/medium)\n",
        "\n",
        "2. IMPROVE EVALUATION:\n",
        "   - Add more benchmark tasks per quadrant\n",
        "   - Use proper evaluation metrics (BLEU, ROUGE, etc.)\n",
        "   - Implement few-shot evaluation\n",
        "\n",
        "3. ENHANCE MASKING:\n",
        "   - Experiment with different masking strategies\n",
        "   - Add semantic masking (mask related concepts)\n",
        "   - Implement dynamic epsilon based on task difficulty\n",
        "\n",
        "4. ANALYSIS:\n",
        "   - Statistical significance testing\n",
        "   - Error analysis and case studies\n",
        "   - Ablation studies on structural hints\n",
        "\n",
        "5. DATASETS TO CONSIDER:\n",
        "   - WikiText-103 for training\n",
        "   - Winograd Schema Challenge\n",
        "   - SQuAD 2.0\n",
        "   - GLUE benchmark tasks\n",
        "   - CommonsenseQA\n",
        "\n",
        "6. TOKENIZATION IMPROVEMENTS:\n",
        "   - Dynamic vocabulary expansion for entities\n",
        "   - Better handling of multi-word entities\n",
        "   - Consistent entity mapping across documents\n",
        "\n",
        "This POC demonstrates the core methodology. The full research would involve\n",
        "more extensive experiments and analysis.\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Enhanced Winograd Schema Challenge with Hugging Face Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class WinogradEvaluator:\n",
        "    \"\"\"\n",
        "    Evaluates Hugging Face models on Winograd Schema Challenge with detailed error analysis.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model_name: str = \"gpt2\"):\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        \n",
        "        # Set pad token if not present\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        \n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model.to(self.device)\n",
        "        \n",
        "        print(f\"Model {model_name} loaded on {self.device}\")\n",
        "    \n",
        "    def evaluate_schema(self, schema: Dict, epsilon: float = 0.0) -> Dict:\n",
        "        \"\"\"\n",
        "        Evaluate a single Winograd schema.\n",
        "        Returns detailed results including confidence scores and reasoning.\n",
        "        \"\"\"\n",
        "        # Apply masking if epsilon > 0\n",
        "        if epsilon > 0:\n",
        "            masked_text = processor.apply_epsilon_masking(schema[\"text\"], epsilon)\n",
        "        else:\n",
        "            masked_text = schema[\"text\"]\n",
        "        \n",
        "        # Create prompt\n",
        "        prompt = f\"{masked_text} {schema['question']} Answer:\"\n",
        "        \n",
        "        # Get scores for each option\n",
        "        option_scores = []\n",
        "        option_details = []\n",
        "        \n",
        "        for option in schema[\"options\"]:\n",
        "            full_prompt = f\"{prompt} {option}\"\n",
        "            \n",
        "            # Tokenize\n",
        "            inputs = self.tokenizer(full_prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "                logits = outputs.logits\n",
        "                \n",
        "                # Get the probability of the last token (the option)\n",
        "                last_token_logits = logits[0, -1, :]\n",
        "                probabilities = torch.softmax(last_token_logits, dim=-1)\n",
        "                \n",
        "                # Get the probability of the option token\n",
        "                option_tokens = self.tokenizer.encode(option, add_special_tokens=False)\n",
        "                if option_tokens:\n",
        "                    option_prob = probabilities[option_tokens[0]].item()\n",
        "                else:\n",
        "                    option_prob = 0.0\n",
        "                \n",
        "                # Also get the max probability for confidence\n",
        "                max_prob = probabilities.max().item()\n",
        "                \n",
        "                option_scores.append(option_prob)\n",
        "                option_details.append({\n",
        "                    \"option\": option,\n",
        "                    \"probability\": option_prob,\n",
        "                    \"max_probability\": max_prob,\n",
        "                    \"prompt\": full_prompt\n",
        "                })\n",
        "        \n",
        "        # Determine prediction\n",
        "        predicted_idx = np.argmax(option_scores)\n",
        "        predicted_answer = schema[\"options\"][predicted_idx]\n",
        "        is_correct = predicted_answer == schema[\"answer\"]\n",
        "        \n",
        "        return {\n",
        "            \"schema_id\": schema[\"id\"],\n",
        "            \"original_text\": schema[\"text\"],\n",
        "            \"masked_text\": masked_text,\n",
        "            \"epsilon\": epsilon,\n",
        "            \"question\": schema[\"question\"],\n",
        "            \"options\": schema[\"options\"],\n",
        "            \"correct_answer\": schema[\"answer\"],\n",
        "            \"predicted_answer\": predicted_answer,\n",
        "            \"is_correct\": is_correct,\n",
        "            \"option_scores\": option_scores,\n",
        "            \"option_details\": option_details,\n",
        "            \"confidence\": max(option_scores),\n",
        "            \"difficulty\": schema[\"difficulty\"],\n",
        "            \"reasoning\": schema[\"reasoning\"]\n",
        "        }\n",
        "    \n",
        "    def evaluate_all_schemas(self, schemas: List[Dict], epsilon: float = 0.0) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Evaluate all schemas with given epsilon.\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        for schema in schemas:\n",
        "            result = self.evaluate_schema(schema, epsilon)\n",
        "            results.append(result)\n",
        "        return results\n",
        "    \n",
        "    def get_error_analysis(self, results: List[Dict]) -> Dict:\n",
        "        \"\"\"\n",
        "        Analyze errors and provide detailed insights.\n",
        "        \"\"\"\n",
        "        errors = [r for r in results if not r[\"is_correct\"]]\n",
        "        correct = [r for r in results if r[\"is_correct\"]]\n",
        "        \n",
        "        analysis = {\n",
        "            \"total_schemas\": len(results),\n",
        "            \"correct_count\": len(correct),\n",
        "            \"error_count\": len(errors),\n",
        "            \"accuracy\": len(correct) / len(results),\n",
        "            \"errors_by_difficulty\": {},\n",
        "            \"error_details\": errors,\n",
        "            \"correct_details\": correct,\n",
        "            \"avg_confidence_correct\": np.mean([r[\"confidence\"] for r in correct]) if correct else 0,\n",
        "            \"avg_confidence_errors\": np.mean([r[\"confidence\"] for r in errors]) if errors else 0\n",
        "        }\n",
        "        \n",
        "        # Analyze errors by difficulty\n",
        "        for difficulty in [\"easy\", \"medium\", \"hard\"]:\n",
        "            difficulty_errors = [e for e in errors if e[\"difficulty\"] == difficulty]\n",
        "            difficulty_total = len([r for r in results if r[\"difficulty\"] == difficulty])\n",
        "            analysis[\"errors_by_difficulty\"][difficulty] = {\n",
        "                \"error_count\": len(difficulty_errors),\n",
        "                \"total_count\": difficulty_total,\n",
        "                \"error_rate\": len(difficulty_errors) / difficulty_total if difficulty_total > 0 else 0\n",
        "            }\n",
        "        \n",
        "        return analysis\n",
        "\n",
        "# Initialize evaluator\n",
        "evaluator = WinogradEvaluator(\"gpt2\")\n",
        "print(\"Winograd evaluator ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Easy Access Functions for Winograd Analysis\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_winograd_experiment(model_name: str = \"gpt2\", epsilon_values: List[float] = [0.0, 0.1, 0.3, 0.5]) -> Dict:\n",
        "    \"\"\"\n",
        "    Run complete Winograd experiment with multiple epsilon values.\n",
        "    \n",
        "    Args:\n",
        "        model_name: Hugging Face model name\n",
        "        epsilon_values: List of masking levels to test\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with results for each epsilon level\n",
        "    \"\"\"\n",
        "    print(f\"Running Winograd experiment with {model_name}\")\n",
        "    print(f\"Testing epsilon values: {epsilon_values}\")\n",
        "    \n",
        "    # Initialize evaluator\n",
        "    evaluator = WinogradEvaluator(model_name)\n",
        "    \n",
        "    # Get all schemas\n",
        "    schemas = winograd_data.get_all_schemas()\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    for epsilon in epsilon_values:\n",
        "        print(f\"\\\\n{'='*20} Testing ε={epsilon} {'='*20}\")\n",
        "        \n",
        "        # Evaluate all schemas\n",
        "        schema_results = evaluator.evaluate_all_schemas(schemas, epsilon)\n",
        "        \n",
        "        # Get error analysis\n",
        "        analysis = evaluator.get_error_analysis(schema_results)\n",
        "        \n",
        "        results[epsilon] = {\n",
        "            \"schema_results\": schema_results,\n",
        "            \"analysis\": analysis\n",
        "        }\n",
        "        \n",
        "        print(f\"Accuracy: {analysis['accuracy']:.3f} ({analysis['correct_count']}/{analysis['total_schemas']})\")\n",
        "        print(f\"Errors: {analysis['error_count']}\")\n",
        "        \n",
        "        # Show error breakdown by difficulty\n",
        "        for difficulty, stats in analysis[\"errors_by_difficulty\"].items():\n",
        "            print(f\"  {difficulty}: {stats['error_count']}/{stats['total_count']} ({stats['error_rate']:.3f})\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "def get_detailed_errors(results: Dict, epsilon: float) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Get detailed information about errors for a specific epsilon.\n",
        "    \n",
        "    Args:\n",
        "        results: Results from run_winograd_experiment\n",
        "        epsilon: Epsilon value to analyze\n",
        "    \n",
        "    Returns:\n",
        "        List of detailed error information\n",
        "    \"\"\"\n",
        "    if epsilon not in results:\n",
        "        print(f\"No results found for epsilon={epsilon}\")\n",
        "        return []\n",
        "    \n",
        "    errors = results[epsilon][\"analysis\"][\"error_details\"]\n",
        "    \n",
        "    detailed_errors = []\n",
        "    for error in errors:\n",
        "        detailed_error = {\n",
        "            \"schema_id\": error[\"schema_id\"],\n",
        "            \"original_text\": error[\"original_text\"],\n",
        "            \"masked_text\": error[\"masked_text\"],\n",
        "            \"question\": error[\"question\"],\n",
        "            \"correct_answer\": error[\"correct_answer\"],\n",
        "            \"predicted_answer\": error[\"predicted_answer\"],\n",
        "            \"difficulty\": error[\"difficulty\"],\n",
        "            \"confidence\": error[\"confidence\"],\n",
        "            \"reasoning\": error[\"reasoning\"],\n",
        "            \"option_scores\": error[\"option_scores\"],\n",
        "            \"option_details\": error[\"option_details\"]\n",
        "        }\n",
        "        detailed_errors.append(detailed_error)\n",
        "    \n",
        "    return detailed_errors\n",
        "\n",
        "def compare_epsilon_performance(results: Dict) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Compare performance across different epsilon values.\n",
        "    \n",
        "    Args:\n",
        "        results: Results from run_winograd_experiment\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with performance comparison\n",
        "    \"\"\"\n",
        "    comparison_data = []\n",
        "    \n",
        "    for epsilon, result in results.items():\n",
        "        analysis = result[\"analysis\"]\n",
        "        \n",
        "        row = {\n",
        "            \"epsilon\": epsilon,\n",
        "            \"accuracy\": analysis[\"accuracy\"],\n",
        "            \"correct_count\": analysis[\"correct_count\"],\n",
        "            \"error_count\": analysis[\"error_count\"],\n",
        "            \"avg_confidence_correct\": analysis[\"avg_confidence_correct\"],\n",
        "            \"avg_confidence_errors\": analysis[\"avg_confidence_errors\"]\n",
        "        }\n",
        "        \n",
        "        # Add difficulty-specific accuracies\n",
        "        for difficulty in [\"easy\", \"medium\", \"hard\"]:\n",
        "            difficulty_stats = analysis[\"errors_by_difficulty\"][difficulty]\n",
        "            row[f\"{difficulty}_accuracy\"] = 1 - difficulty_stats[\"error_rate\"]\n",
        "        \n",
        "        comparison_data.append(row)\n",
        "    \n",
        "    return pd.DataFrame(comparison_data)\n",
        "\n",
        "def visualize_winograd_results(results: Dict):\n",
        "    \"\"\"\n",
        "    Create visualizations of the Winograd results.\n",
        "    \"\"\"\n",
        "    df = compare_epsilon_performance(results)\n",
        "    \n",
        "    # Create subplots\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # Overall accuracy\n",
        "    axes[0, 0].plot(df['epsilon'], df['accuracy'], marker='o', linewidth=2)\n",
        "    axes[0, 0].set_xlabel('Epsilon (Masking Level)')\n",
        "    axes[0, 0].set_ylabel('Accuracy')\n",
        "    axes[0, 0].set_title('Winograd Accuracy vs. Masking Level')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Accuracy by difficulty\n",
        "    for difficulty in ['easy', 'medium', 'hard']:\n",
        "        axes[0, 1].plot(df['epsilon'], df[f'{difficulty}_accuracy'], marker='o', label=difficulty.title(), linewidth=2)\n",
        "    axes[0, 1].set_xlabel('Epsilon (Masking Level)')\n",
        "    axes[0, 1].set_ylabel('Accuracy')\n",
        "    axes[0, 1].set_title('Winograd Accuracy by Difficulty')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Confidence comparison\n",
        "    axes[1, 0].plot(df['epsilon'], df['avg_confidence_correct'], marker='o', label='Correct', linewidth=2)\n",
        "    axes[1, 0].plot(df['epsilon'], df['avg_confidence_errors'], marker='s', label='Errors', linewidth=2)\n",
        "    axes[1, 0].set_xlabel('Epsilon (Masking Level)')\n",
        "    axes[1, 0].set_ylabel('Average Confidence')\n",
        "    axes[1, 0].set_title('Confidence: Correct vs. Errors')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Error count\n",
        "    axes[1, 1].bar(df['epsilon'], df['error_count'], alpha=0.7)\n",
        "    axes[1, 1].set_xlabel('Epsilon (Masking Level)')\n",
        "    axes[1, 1].set_ylabel('Number of Errors')\n",
        "    axes[1, 1].set_title('Error Count by Masking Level')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return df\n",
        "\n",
        "print(\"Easy access functions ready!\")\n",
        "print(\"\\\\nAvailable functions:\")\n",
        "print(\"- run_winograd_experiment(model_name, epsilon_values)\")\n",
        "print(\"- get_detailed_errors(results, epsilon)\")\n",
        "print(\"- compare_epsilon_performance(results)\")\n",
        "print(\"- visualize_winograd_results(results)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "deep",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
