{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Notebook\n",
        "\n",
        "Concise training pipeline for Knowledge vs Reasoning Separation project.\n",
        "\n",
        "**Purpose**: Train models with different Œµ-masking values and save results to GCS for evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "from typing import List, Dict, Optional\n",
        "import os\n",
        "\n",
        "# GCS imports\n",
        "try:\n",
        "    import gcsfs\n",
        "    import pyarrow.parquet as pq\n",
        "    GCS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    GCS_AVAILABLE = False\n",
        "    print(\"Warning: GCS not available\")\n",
        "\n",
        "print(\"‚úÖ Imports successful\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "EPSILON_VALUES = [0.0, 0.1, 0.3, 0.5, 0.7]  # 5 different epsilon values\n",
        "MODEL_NAME = \"gpt2\"\n",
        "BUCKET_NAME = \"parquet_v2_openwebtext-with-pos-ner\"\n",
        "OUTPUT_BUCKET = \"model-training-results\"  # Where to save trained models\n",
        "CREDENTIALS_PATH = \"eastern-bridge-credentials.json\"\n",
        "\n",
        "# Training parameters\n",
        "MAX_LENGTH = 512\n",
        "BATCH_SIZE = 8\n",
        "LEARNING_RATE = 5e-5\n",
        "NUM_EPOCHS = 1\n",
        "MAX_SAMPLES = 1000  # Limit for quick training\n",
        "\n",
        "print(f\"Training configuration:\")\n",
        "print(f\"  Epsilon values: {EPSILON_VALUES}\")\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "print(f\"  Max samples: {MAX_SAMPLES}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GCS Data Loader\n",
        "class GCSDataLoader:\n",
        "    def __init__(self, bucket_name: str, credentials_path: str):\n",
        "        if not GCS_AVAILABLE:\n",
        "            raise ImportError(\"GCS not available\")\n",
        "        \n",
        "        self.fs = gcsfs.GCSFileSystem(token=credentials_path)\n",
        "        self.bucket_name = bucket_name\n",
        "        print(f\"‚úÖ Connected to GCS bucket: {bucket_name}\")\n",
        "    \n",
        "    def load_sample_data(self, max_samples: int = 1000) -> pd.DataFrame:\n",
        "        \"\"\"Load sample data for training.\"\"\"\n",
        "        try:\n",
        "            # List parquet files\n",
        "            files = sorted(self.fs.glob(f\"{self.bucket_name}/**/*.parquet\"))\n",
        "            print(f\"Found {len(files)} files\")\n",
        "            \n",
        "            if not files:\n",
        "                raise ValueError(\"No parquet files found\")\n",
        "            \n",
        "            # Load first few files\n",
        "            dfs = []\n",
        "            total_samples = 0\n",
        "            \n",
        "            for file_path in files[:5]:  # Load first 5 files\n",
        "                if total_samples >= max_samples:\n",
        "                    break\n",
        "                    \n",
        "                df = pd.read_parquet(f\"gs://{file_path}\")\n",
        "                dfs.append(df)\n",
        "                total_samples += len(df)\n",
        "                print(f\"Loaded {len(df)} samples from {file_path}\")\n",
        "            \n",
        "            combined_df = pd.concat(dfs, ignore_index=True)\n",
        "            # Sample if we have too many\n",
        "            if len(combined_df) > max_samples:\n",
        "                combined_df = combined_df.sample(n=max_samples, random_state=42)\n",
        "            \n",
        "            print(f\"‚úÖ Loaded {len(combined_df)} total samples\")\n",
        "            return combined_df\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading data: {e}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "# Initialize data loader\n",
        "if GCS_AVAILABLE:\n",
        "    data_loader = GCSDataLoader(BUCKET_NAME, CREDENTIALS_PATH)\n",
        "else:\n",
        "    data_loader = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Œµ-Masking Function (simplified from tokenization notebook)\n",
        "def apply_epsilon_masking(text: str, epsilon: float) -> str:\n",
        "    \"\"\"Apply Œµ-masking to text.\"\"\"\n",
        "    if epsilon == 0.0:\n",
        "        return text\n",
        "    \n",
        "    words = text.split()\n",
        "    masked_words = []\n",
        "    \n",
        "    for word in words:\n",
        "        # Simple masking - mask content words with probability epsilon\n",
        "        if np.random.random() < epsilon and len(word) > 2:\n",
        "            masked_words.append(\"<mask>\")\n",
        "        else:\n",
        "            masked_words.append(word)\n",
        "    \n",
        "    return \" \".join(masked_words)\n",
        "\n",
        "# Tokenizer setup\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenize examples for training.\"\"\"\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "print(\"‚úÖ Masking function and tokenizer ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Function\n",
        "def train_model_for_epsilon(epsilon: float, train_dataset: Dataset) -> Dict:\n",
        "    \"\"\"Train a model for a specific epsilon value.\"\"\"\n",
        "    print(f\"\\nüöÄ Training model for Œµ = {epsilon}\")\n",
        "    \n",
        "    # Apply masking to dataset\n",
        "    masked_texts = [apply_epsilon_masking(text, epsilon) for text in train_dataset[\"text\"]]\n",
        "    masked_dataset = Dataset.from_dict({\"text\": masked_texts})\n",
        "    \n",
        "    # Tokenize dataset\n",
        "    tokenized_dataset = masked_dataset.map(tokenize_function, batched=True)\n",
        "    \n",
        "    # Initialize model\n",
        "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "    \n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./model_epsilon_{epsilon}\",\n",
        "        num_train_epochs=NUM_EPOCHS,\n",
        "        per_device_train_batch_size=BATCH_SIZE,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        logging_steps=10,\n",
        "        save_steps=100,\n",
        "        evaluation_strategy=\"no\",\n",
        "        save_total_limit=1,\n",
        "    )\n",
        "    \n",
        "    # Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "    \n",
        "    # Train\n",
        "    start_time = time.time()\n",
        "    trainer.train()\n",
        "    training_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"‚úÖ Training completed for Œµ = {epsilon} in {training_time:.2f}s\")\n",
        "    \n",
        "    return {\n",
        "        \"epsilon\": epsilon,\n",
        "        \"model\": model,\n",
        "        \"trainer\": trainer,\n",
        "        \"training_time\": training_time,\n",
        "        \"num_samples\": len(train_dataset)\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Training function ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GCS Model Saver\n",
        "class GCSModelSaver:\n",
        "    def __init__(self, bucket_name: str, credentials_path: str):\n",
        "        if not GCS_AVAILABLE:\n",
        "            raise ImportError(\"GCS not available\")\n",
        "        \n",
        "        self.fs = gcsfs.GCSFileSystem(token=credentials_path)\n",
        "        self.bucket_name = bucket_name\n",
        "        print(f\"‚úÖ Model saver connected to bucket: {bucket_name}\")\n",
        "    \n",
        "    def save_model_results(self, results: List[Dict]):\n",
        "        \"\"\"Save training results to GCS.\"\"\"\n",
        "        try:\n",
        "            # Create results summary\n",
        "            summary = {\n",
        "                \"training_timestamp\": time.time(),\n",
        "                \"model_name\": MODEL_NAME,\n",
        "                \"epsilon_values\": EPSILON_VALUES,\n",
        "                \"training_config\": {\n",
        "                    \"max_length\": MAX_LENGTH,\n",
        "                    \"batch_size\": BATCH_SIZE,\n",
        "                    \"learning_rate\": LEARNING_RATE,\n",
        "                    \"num_epochs\": NUM_EPOCHS,\n",
        "                    \"max_samples\": MAX_SAMPLES\n",
        "                },\n",
        "                \"results\": []\n",
        "            }\n",
        "            \n",
        "            for result in results:\n",
        "                summary[\"results\"].append({\n",
        "                    \"epsilon\": result[\"epsilon\"],\n",
        "                    \"training_time\": result[\"training_time\"],\n",
        "                    \"num_samples\": result[\"num_samples\"]\n",
        "                })\n",
        "            \n",
        "            # Save summary to GCS\n",
        "            summary_path = f\"{self.bucket_name}/training_summary_{int(time.time())}.json\"\n",
        "            with self.fs.open(summary_path, 'w') as f:\n",
        "                json.dump(summary, f, indent=2)\n",
        "            \n",
        "            print(f\"‚úÖ Training summary saved to: {summary_path}\")\n",
        "            return summary_path\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error saving results: {e}\")\n",
        "            return None\n",
        "\n",
        "# Initialize model saver\n",
        "if GCS_AVAILABLE:\n",
        "    model_saver = GCSModelSaver(OUTPUT_BUCKET, CREDENTIALS_PATH)\n",
        "else:\n",
        "    model_saver = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main Training Pipeline\n",
        "def run_training_pipeline():\n",
        "    \"\"\"Run the complete training pipeline.\"\"\"\n",
        "    print(\"üöÄ Starting Training Pipeline\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Load data\n",
        "    if data_loader is None:\n",
        "        print(\"‚ùå GCS not available - using dummy data\")\n",
        "        # Create dummy data for testing\n",
        "        dummy_texts = [\n",
        "            \"The quick brown fox jumps over the lazy dog.\",\n",
        "            \"Machine learning is transforming artificial intelligence.\",\n",
        "            \"Natural language processing enables computers to understand text.\",\n",
        "            \"Deep learning models require large amounts of training data.\",\n",
        "            \"Transformers have revolutionized the field of NLP.\"\n",
        "        ] * 200  # Repeat to get 1000 samples\n",
        "        train_data = pd.DataFrame({\"text\": dummy_texts[:MAX_SAMPLES]})\n",
        "    else:\n",
        "        train_data = data_loader.load_sample_data(MAX_SAMPLES)\n",
        "    \n",
        "    if train_data.empty:\n",
        "        print(\"‚ùå No training data available\")\n",
        "        return\n",
        "    \n",
        "    # Convert to Dataset\n",
        "    train_dataset = Dataset.from_pandas(train_data)\n",
        "    print(f\"‚úÖ Training dataset ready: {len(train_dataset)} samples\")\n",
        "    \n",
        "    # Train models for each epsilon value\n",
        "    results = []\n",
        "    for epsilon in EPSILON_VALUES:\n",
        "        try:\n",
        "            result = train_model_for_epsilon(epsilon, train_dataset)\n",
        "            results.append(result)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Training failed for Œµ = {epsilon}: {e}\")\n",
        "    \n",
        "    # Save results\n",
        "    if model_saver and results:\n",
        "        summary_path = model_saver.save_model_results(results)\n",
        "        print(f\"\\n‚úÖ Training pipeline completed!\")\n",
        "        print(f\"üìä Trained {len(results)} models\")\n",
        "        print(f\"üíæ Results saved to: {summary_path}\")\n",
        "    else:\n",
        "        print(f\"\\n‚úÖ Training pipeline completed!\")\n",
        "        print(f\"üìä Trained {len(results)} models\")\n",
        "        print(\"‚ö†Ô∏è Results not saved to GCS\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run the pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    results = run_training_pipeline()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "deep",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
