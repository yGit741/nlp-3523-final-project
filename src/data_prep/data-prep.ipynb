{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9af1d2cd",
      "metadata": {
        "id": "9af1d2cd"
      },
      "source": [
        "Data preparation pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_Wdj0Q6DvHGA",
      "metadata": {
        "id": "_Wdj0Q6DvHGA"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h5cTC9EQvYgc",
      "metadata": {
        "id": "h5cTC9EQvYgc"
      },
      "outputs": [],
      "source": [
        "from googleapiclient.discovery import build\n",
        "from google.colab import userdata\n",
        "from google.cloud import storage, drive\n",
        "\n",
        "\n",
        "gcs_service = build('storage', 'v1')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EicykZ4Mxdvd",
      "metadata": {
        "id": "EicykZ4Mxdvd"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E_he16XxpWQO",
      "metadata": {
        "id": "E_he16XxpWQO"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "class BaseSaveDriver(ABC):\n",
        "    \"\"\"\n",
        "    Abstract base class for save drivers.\n",
        "    Allows different storage implementations (local, cloud, etc.).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, batch_size: int = 100):\n",
        "        \"\"\"\n",
        "        Initialize the base save driver.\n",
        "\n",
        "        Args:\n",
        "            batch_size: Number of documents per batch\n",
        "        \"\"\"\n",
        "        self.batch_size = batch_size\n",
        "        self.current_batch = []\n",
        "        self.batch_count = 0\n",
        "        self.documents_processed = 0\n",
        "\n",
        "    @abstractmethod\n",
        "    def add_document(self, document):\n",
        "        \"\"\"Add a document to the current batch.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def finalize(self):\n",
        "        \"\"\"Save any remaining documents and return statistics.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_statistics(self):\n",
        "        \"\"\"Get current statistics.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def _save_current_batch(self):\n",
        "        \"\"\"Abstract method to save the current batch to storage.\"\"\"\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c72c63d3",
      "metadata": {
        "id": "c72c63d3"
      },
      "outputs": [],
      "source": [
        "from spacy.pipeline import EntityRuler\n",
        "import spacy\n",
        "import re\n",
        "from typing import List, Dict, Any, Iterator\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import tempfile\n",
        "import uuid\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "\n",
        "class SpacyJSONGenerator:\n",
        "    def __init__(self, batch_size: int = 100, n_process: int = 1, require_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Initialize the generator with batching capabilities.\n",
        "\n",
        "        Args:\n",
        "            batch_size: Number of texts (sentences) to process in each batch\n",
        "            n_process: Number of processes for parallel processing (use -1 for all cores)\n",
        "        \"\"\"\n",
        "        # Load the transformer model\n",
        "        if require_gpu:\n",
        "            spacy.require_gpu()\n",
        "        self.nlp = spacy.load(\"en_core_web_trf\", disable=[\"lemmatizer\"])\n",
        "        self.batch_size = batch_size\n",
        "        self.n_process = n_process\n",
        "\n",
        "        # Add EntityRuler for NLE extraction (BEFORE NER for better integration)\n",
        "        ruler = self.nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
        "        self._setup_nle_patterns(ruler)\n",
        "\n",
        "    def _setup_nle_patterns(self, ruler: EntityRuler):\n",
        "        \"\"\"Setup patterns for Nonlinguistic Entity extraction using EntityRuler.\"\"\"\n",
        "        patterns = [\n",
        "            # Phone patterns\n",
        "            {\"label\": \"PHONE\", \"pattern\": [{\"TEXT\": {\"REGEX\": r\"\\(?[0-9]{3}\\)?[-.\\s]?[0-9]{3}[-.\\s]?[0-9]{4}\"}}]},\n",
        "            {\"label\": \"PHONE\", \"pattern\": [{\"TEXT\": {\"REGEX\": r\"\\+[1-9]\\d{1,14}\"}}]},\n",
        "\n",
        "            # Address patterns\n",
        "            {\"label\": \"ADDRESS\", \"pattern\": [{\"IS_DIGIT\": True}, {\"IS_ALPHA\": True, \"OP\": \"+\"}, {\"LOWER\": {\"IN\": [\"st\", \"street\", \"ave\", \"avenue\", \"rd\", \"road\", \"blvd\", \"boulevard\", \"dr\", \"drive\", \"ln\", \"lane\", \"ct\", \"court\", \"pl\", \"place\"]}}]},\n",
        "            {\"label\": \"ADDRESS\", \"pattern\": [{\"LOWER\": \"p\"}, {\"TEXT\": \".\"}, {\"LOWER\": \"o\"}, {\"TEXT\": \".\"}, {\"LOWER\": \"box\"}, {\"IS_DIGIT\": True}]},\n",
        "\n",
        "            # IP Address patterns\n",
        "            {\"label\": \"IP_ADDRESS\", \"pattern\": [{\"TEXT\": {\"REGEX\": r\"\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b\"}}]},\n",
        "            {\"label\": \"IP_ADDRESS\", \"pattern\": [{\"TEXT\": {\"REGEX\": r\"\\b(?:[0-9a-fA-F]{1,4}:){7}[0-9a-fA-F]{1,4}\\b\"}}]},\n",
        "\n",
        "            # SSN patterns\n",
        "            {\"label\": \"SSN\", \"pattern\": [{\"IS_DIGIT\": True, \"LENGTH\": 3}, {\"TEXT\": \"-\"}, {\"IS_DIGIT\": True, \"LENGTH\": 2}, {\"TEXT\": \"-\"}, {\"IS_DIGIT\": True, \"LENGTH\": 4}]},\n",
        "            {\"label\": \"SSN\", \"pattern\": [{\"IS_DIGIT\": True, \"LENGTH\": 3}, {\"IS_SPACE\": True}, {\"IS_DIGIT\": True, \"LENGTH\": 2}, {\"IS_SPACE\": True}, {\"IS_DIGIT\": True, \"LENGTH\": 4}]},\n",
        "\n",
        "            # URL and Email patterns (using built-ins)\n",
        "            {\"label\": \"URL\", \"pattern\": [{\"LIKE_URL\": True}]},\n",
        "            {\"label\": \"EMAIL\", \"pattern\": [{\"LIKE_EMAIL\": True}]}\n",
        "        ]\n",
        "\n",
        "        ruler.add_patterns(patterns)\n",
        "\n",
        "\n",
        "    def _extract_punctuation_spans(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Extract punctuation spans from text.\"\"\"\n",
        "        punct_spans = []\n",
        "        punct_pattern = r'[^\\w\\s]'  # Match non-word, non-space characters\n",
        "\n",
        "        for match in re.finditer(punct_pattern, text):\n",
        "            punct_spans.append({\n",
        "                \"start\": match.start(),\n",
        "                \"end\": match.end(),\n",
        "                \"value\": match.group()\n",
        "            })\n",
        "\n",
        "        return punct_spans\n",
        "\n",
        "    def _extract_special_tags_from_doc(self, doc) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Extract special tags from spaCy doc (NLEs are now in doc.ents).\"\"\"\n",
        "        special_tags = []\n",
        "\n",
        "        # Filter NLE entities (non-standard NER labels)\n",
        "        nle_labels = {\"PHONE\", \"ADDRESS\", \"IP_ADDRESS\", \"SSN\", \"URL\", \"EMAIL\"}\n",
        "\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ in nle_labels:\n",
        "                special_tags.append({\n",
        "                    \"start\": ent.start_char,\n",
        "                    \"end\": ent.end_char,\n",
        "                    \"type\": ent.label_,\n",
        "                    \"value\": ent.text\n",
        "                })\n",
        "\n",
        "        return special_tags\n",
        "\n",
        "    def _get_sentence_spans(self, doc, text: str) -> List[Dict[str, int]]:\n",
        "        \"\"\"Extract sentence spans.\"\"\"\n",
        "        sent_spans = []\n",
        "        for sent in doc.sents:\n",
        "            sent_spans.append({\n",
        "                \"start\": sent.start_char,\n",
        "                \"end\": sent.end_char\n",
        "            })\n",
        "        return sent_spans\n",
        "\n",
        "    def process_single_doc(self, doc, original_text: str, sentence_id: str) -> Dict[str, Any]:\n",
        "        \"\"\"Process a single spaCy doc and return the JSON structure.\"\"\"\n",
        "\n",
        "        # Extract sentence spans\n",
        "        sent_spans = self._get_sentence_spans(doc, original_text)\n",
        "\n",
        "        # Extract punctuation spans\n",
        "        punct_spans = self._extract_punctuation_spans(original_text)\n",
        "\n",
        "        # Extract special tags (NLEs) from doc.ents\n",
        "        special_tags = self._extract_special_tags_from_doc(doc)\n",
        "\n",
        "        # Extract named entity spans with entity IDs (standard NER only)\n",
        "        ner_spans = []\n",
        "        nle_labels = {\"PHONE\", \"ADDRESS\", \"IP_ADDRESS\", \"SSN\", \"URL\", \"EMAIL\"}\n",
        "\n",
        "        for ent in doc.ents:\n",
        "            # Only include standard NER entities, not NLEs\n",
        "            if ent.label_ not in nle_labels:\n",
        "                ner_spans.append({\n",
        "                    \"entity_id\": f\"{ent.label_}-{str(ent).upper().replace(' ', '_').replace('-', '_')}\",\n",
        "                    \"start\": ent.start_char,\n",
        "                    \"end\": ent.end_char,\n",
        "                    \"label\": ent.label_\n",
        "                })\n",
        "\n",
        "        # Extract POS tokens and tags\n",
        "        pos_tokens = []\n",
        "        pos_tags = []\n",
        "        ner_iob = []\n",
        "\n",
        "        for token in doc:\n",
        "            # Skip whitespace-only tokens\n",
        "            if not token.text.strip():\n",
        "                continue\n",
        "\n",
        "            pos_tokens.append(token.text)\n",
        "            pos_tags.append(token.pos_)\n",
        "\n",
        "            # Determine IOB tag\n",
        "            if token.ent_iob_ == 'B':\n",
        "                ner_iob.append(f\"B-{token.ent_type_}\")\n",
        "            elif token.ent_iob_ == 'I':\n",
        "                ner_iob.append(f\"I-{token.ent_type_}\")\n",
        "            else:\n",
        "                ner_iob.append(\"O\")\n",
        "\n",
        "        # Build the final JSON structure\n",
        "        result = {\n",
        "            \"id\": sentence_id,\n",
        "            \"text\": original_text,\n",
        "            \"sent_spans\": sent_spans,\n",
        "            \"punct_spans\": punct_spans,\n",
        "            \"special_tags\": special_tags,\n",
        "            \"ner_spans\": ner_spans,\n",
        "            \"pos_tokens\": pos_tokens,\n",
        "            \"pos_tags\": pos_tags,\n",
        "            \"ner_iob\": ner_iob\n",
        "        }\n",
        "\n",
        "        return result\n",
        "\n",
        "    def process_sentences_batch(self, sentences: List[str], sentence_ids: List[str] = None):\n",
        "        \"\"\"Process a batch of sentences efficiently.\"\"\"\n",
        "        if sentence_ids is None:\n",
        "            sentence_ids = [f\"sent_{str(uuid.uuid4())}\" for _ in range(len(sentences))]\n",
        "\n",
        "        # Process batch with spaCy\n",
        "        docs = list(self.nlp.pipe(sentences, batch_size=self.batch_size, n_process=self.n_process))\n",
        "\n",
        "        # Process each doc\n",
        "        results = []\n",
        "        for doc, original_text, sent_id in zip(docs, sentences, sentence_ids):\n",
        "            result = self.process_single_doc(doc, original_text, sent_id)\n",
        "            results.append(result)\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "    def process_sentences_streaming(self, sentences: Iterator[str],\n",
        "                                   sentence_id_generator: Iterator[str] = None) -> Iterator[Dict[str, Any]]:\n",
        "        \"\"\"Process sentences in streaming fashion with batching.\"\"\"\n",
        "        sentence_batch = []\n",
        "        id_batch = []\n",
        "\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            sentence_batch.append(sentence)\n",
        "\n",
        "            if sentence_id_generator:\n",
        "                id_batch.append(next(sentence_id_generator))\n",
        "            else:\n",
        "                id_batch.append(f\"sent_{i:07d}\")\n",
        "\n",
        "            # Process batch when it reaches batch_size\n",
        "            if len(sentence_batch) >= self.batch_size:\n",
        "                results = self.process_sentences_batch(sentence_batch, id_batch)\n",
        "                for result in results:\n",
        "                    yield result\n",
        "\n",
        "                # Clear batches\n",
        "                sentence_batch = []\n",
        "                id_batch = []\n",
        "\n",
        "        # Process remaining sentences\n",
        "        if sentence_batch:\n",
        "            results = self.process_sentences_batch(sentence_batch)\n",
        "            for result in results:\n",
        "                yield result\n",
        "\n",
        "    def process_and_save(self, dataset, save_driver: BaseSaveDriver, num_batches=None, resume_from_progress=True):\n",
        "        \"\"\"\n",
        "        Process dataset using Hugging Face map() function with configurable save driver.\n",
        "        Includes detailed timing measurements and bottleneck analysis.\n",
        "\n",
        "        Args:\n",
        "            dataset: Hugging Face dataset\n",
        "            save_driver: SaveDriver instance for handling storage (local, cloud, etc.)\n",
        "            num_batches: Number of batches to process (None = process all)\n",
        "            resume_from_progress: Whether to resume from existing progress (if available)\n",
        "\n",
        "        Returns:\n",
        "            BaseSaveDriver: The save driver instance with statistics\n",
        "        \"\"\"\n",
        "\n",
        "        print(f\"🚀 Starting HF map() optimized processing with {save_driver.__class__.__name__}...\")\n",
        "\n",
        "        # Check if we should resume from existing progress\n",
        "        documents_to_skip = 0\n",
        "        initial_batch_count = 0\n",
        "        if resume_from_progress and hasattr(save_driver, 'progress_data'):\n",
        "            progress = save_driver.progress_data\n",
        "            if progress['documents_processed'] > 0:\n",
        "                documents_to_skip = progress['documents_processed']\n",
        "                initial_batch_count = progress['batch_count']\n",
        "\n",
        "                print(f\"🔄 Resuming from previous progress:\")\n",
        "                print(f\"   📄 Documents already processed: {progress['documents_processed']}\")\n",
        "                print(f\"   📦 Batches already created: {progress['batch_count']}\")\n",
        "                print(f\"⏭️  Skipping first {documents_to_skip} documents...\")\n",
        "\n",
        "        def process_batch_texts(batch):\n",
        "            \"\"\"Process a batch of texts with spaCy using HF map.\"\"\"\n",
        "\n",
        "            texts = [text for text in batch['text'] if len(text) >= 10]\n",
        "\n",
        "            if not texts:\n",
        "                return {'processed': [None] * len(batch['text'])}\n",
        "\n",
        "            try:\n",
        "                processed_docs = self.process_sentences_batch(texts)\n",
        "                return {'processed': processed_docs}\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error processing batch: {e}\")\n",
        "                return {'processed': [None] * len(batch['text'])}\n",
        "\n",
        "        print(f\"Skipping documents (if needed) and adding mapping\")\n",
        "        processed_dataset = dataset['train'].skip(documents_to_skip * self.batch_size).map(\n",
        "            process_batch_texts,\n",
        "            batched=True,\n",
        "            batch_size=self.batch_size,\n",
        "            remove_columns=['text']\n",
        "        )\n",
        "\n",
        "        print(\"💾 Processing and saving data...\")\n",
        "\n",
        "        processed_count = 0\n",
        "\n",
        "        try:\n",
        "            for example in processed_dataset:\n",
        "\n",
        "\n",
        "                save_driver.add_document(example['processed'])\n",
        "                processed_count += 1\n",
        "\n",
        "                # Check batch count more frequently to respect num_batches limit\n",
        "                current_batch_count = save_driver.batch_count\n",
        "                new_batches_created = current_batch_count - initial_batch_count\n",
        "\n",
        "                # Check if we've processed enough NEW batches (check after each document)\n",
        "                if num_batches is not None and new_batches_created >= num_batches:\n",
        "                    print(f\"🛑 Reached target of {num_batches} new batches. Stopping...\")\n",
        "                    print(f\"   📊 Total batches: {current_batch_count}, New batches this run: {new_batches_created}\")\n",
        "                    break\n",
        "\n",
        "\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n⚠️  Processing interrupted by user. Progress saved.\")\n",
        "            if hasattr(save_driver, '_save_progress'):\n",
        "                save_driver._save_progress()\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            print(f\"\\n❌ Processing failed: {e}\")\n",
        "            print(\"💾 Progress saved. You can resume later.\")\n",
        "            if hasattr(save_driver, '_save_progress'):\n",
        "                save_driver._save_progress()\n",
        "            raise\n",
        "\n",
        "\n",
        "        # Finalize and get statistics\n",
        "        batch_count, documents_processed = save_driver.finalize()\n",
        "\n",
        "        # Calculate total time and performance metrics\n",
        "\n",
        "        print(f\"\\n🎉 Processing completed!\")\n",
        "        print(f\"📊 Performance Summary:\")\n",
        "        print(f\"   📄 Documents processed: {documents_processed}\")\n",
        "        print(f\"   📦 Batches created: {batch_count}\")\n",
        "\n",
        "        return save_driver\n",
        "\n",
        "class CloudSaveDriver(BaseSaveDriver):\n",
        "    \"\"\"\n",
        "    Google Cloud Storage (GCS) implementation for saving processed batches.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, bucket_name=None, project_id=None, batch_size=100, progress_file=\"gcs_processing_progress.json\"):\n",
        "        \"\"\"\n",
        "        Initialize the CloudSaveDriver with GCS support.\n",
        "        \n",
        "        Args:\n",
        "            bucket_name: GCS bucket name (optional, can be set in config)\n",
        "            batch_size: Number of documents per batch file\n",
        "            progress_file: File to store processing progress for resumption\n",
        "        \"\"\"\n",
        "        super().__init__(batch_size)\n",
        "        self.progress_path = '/content/drive/My Drive/nlp-3523-final-project/data_prep/' + progress_file\n",
        "        self.colab_drive = drive\n",
        "        self._mount_drive()\n",
        "        self.progress_data = self._load_progress()\n",
        "        \n",
        "        # Import here to avoid dependency issues if GCS not installed\n",
        "        try:\n",
        "            from google.cloud import storage\n",
        "            from google.cloud.exceptions import GoogleCloudError\n",
        "        except ImportError as e:\n",
        "            raise ImportError(f\"GCS dependencies not installed. Run: pip install google-cloud-storage. Error: {e}\")\n",
        "        # Store imports for use in methods\n",
        "        self.storage = storage\n",
        "        self.GoogleCloudError = GoogleCloudError\n",
        "        # Validate and get GCS configuration\n",
        "        try:\n",
        "            self.bucket_name = bucket_name\n",
        "            self.project_id = project_id\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"GCS configuration error: {e}\")\n",
        "        # Initialize GCS client\n",
        "        try:\n",
        "            # Credentials from file path\n",
        "\n",
        "            self.client  = storage.Client()\n",
        "\n",
        "            # Get bucket reference\n",
        "            self.bucket = self.client.bucket(self.bucket_name)\n",
        "            \n",
        "            # Test bucket access\n",
        "            if not self.bucket.exists():\n",
        "                raise ValueError(f\"Bucket '{self.bucket_name}' does not exist or is not accessible\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to initialize GCS client: {e}\")\n",
        "        \n",
        "        print(f\"☁️  CloudSaveDriver (GCS) initialized:\")\n",
        "        print(f\"  - Bucket: {self.bucket_name}\")\n",
        "        print(f\"  - Project: {self.project_id}\")\n",
        "        print(f\"  - Batch size: {self.batch_size}\")\n",
        "        \n",
        "        # Restore state from progress file\n",
        "        if self.progress_data['documents_processed'] > 0:\n",
        "            self.documents_processed = self.progress_data['documents_processed']\n",
        "            self.batch_count = self.progress_data['batch_count']\n",
        "            print(f\"  - Resuming from: {self.documents_processed} docs, {self.batch_count} batches\")\n",
        "    \n",
        "    def _mount_drive(self):\n",
        "        \"\"\"Mount the drive.\"\"\"\n",
        "        if not os.path.exists('/content/drive/My Drive'):\n",
        "            print(\"🔗 Mounting Google Drive...\")\n",
        "            self.colab_drive.mount('/content/drive')\n",
        "            print(\"✅ Google Drive mounted successfully\")\n",
        "        else:\n",
        "            print(\"✅ Google Drive already mounted\")\n",
        "        \n",
        "    def _load_progress(self):\n",
        "        \"\"\"Load existing progress if available.\"\"\"\n",
        "        if os.path.exists(self.progress_path):\n",
        "            try:\n",
        "                with open(self.progress_path, 'r') as f:\n",
        "                    progress = json.load(f)\n",
        "                    print(f\"📋 Loaded existing progress: {progress['documents_processed']} docs, {progress['batch_count']} batches\")\n",
        "                    return progress\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️  Could not load progress file: {e}\")\n",
        "        \n",
        "        return {\n",
        "            'documents_processed': 0,\n",
        "            'batch_count': 0,\n",
        "            'start_time': time.time()\n",
        "        }\n",
        "    \n",
        "    def _save_progress(self):\n",
        "        \"\"\"Save current progress to file.\"\"\"\n",
        "        self.progress_data.update({\n",
        "            'documents_processed': self.documents_processed,\n",
        "            'batch_count': self.batch_count,\n",
        "            'last_save_time': time.time()\n",
        "        })\n",
        "        \n",
        "        try:\n",
        "            with open(self.progress_path, 'w') as f:\n",
        "                json.dump(self.progress_data, f, indent=2)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️  Could not save progress: {e}\")\n",
        "    \n",
        "    def add_document(self, document):\n",
        "        \"\"\"\n",
        "        Add a document to the current batch.\n",
        "        \n",
        "        Args:\n",
        "            document: Processed document to add\n",
        "        \"\"\"\n",
        "        if document is not None:\n",
        "            self.current_batch.append(document)\n",
        "            self.documents_processed += 1\n",
        "            \n",
        "            # Save batch when it reaches the desired size\n",
        "            if len(self.current_batch) >= self.batch_size:\n",
        "                self._save_current_batch()\n",
        "    \n",
        "    def _save_current_batch(self):\n",
        "        \"\"\"\n",
        "        Save the current batch to GCS bucket.\n",
        "        \"\"\"\n",
        "        if not self.current_batch:\n",
        "            return\n",
        "        \n",
        "        save_start = time.time()\n",
        "        self.batch_count += 1\n",
        "        \n",
        "        # Create filename with timestamp and batch number\n",
        "        timestamp = int(time.time())\n",
        "        filename = f\"batch_{self.batch_count:06d}_{timestamp}.json\"\n",
        "        \n",
        "        try:\n",
        "            # Create temporary file for JSON data\n",
        "            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as temp_file:\n",
        "                json.dump(self.current_batch, temp_file, ensure_ascii=False)\n",
        "                temp_file_path = temp_file.name\n",
        "            \n",
        "            blob = self.bucket.blob(filename)\n",
        "            \n",
        "            for attempt in range(self.GCS_RETRY_ATTEMPTS):\n",
        "                try:\n",
        "                    blob.upload_from_filename(temp_file_path, content_type='application/json')\n",
        "                    \n",
        "                    break\n",
        "                except self.GoogleCloudError as e:\n",
        "                    if attempt == self.GCS_RETRY_ATTEMPTS - 1:\n",
        "                        raise\n",
        "                    print(f\"⚠️  Upload attempt {attempt + 1} failed, retrying... Error: {e}\")\n",
        "                    time.sleep(2 ** attempt)  # Exponential backoff\n",
        "\n",
        "                # Configure upload settings for large files\n",
        "                # blob.chunk_size = self.config.GCS_UPLOAD_CHUNK_SIZE # default is 100MB\n",
        "                \n",
        "                # Upload with retry logic            \n",
        "            \n",
        "            os.unlink(temp_file_path)  # Clean up\n",
        "            \n",
        "            save_time = time.time() - save_start\n",
        "            file_size_mb = len(json.dumps(self.current_batch)) / 1024 / 1024\n",
        "            \n",
        "            print(f\"☁️  Saved batch {self.batch_count} with {len(self.current_batch)} documents to gs://{self.bucket_name}/{filename}\")\n",
        "            print(f\"   ⏱️  Upload time: {save_time:.3f}s, Size: {file_size_mb:.1f} MB, Rate: {len(self.current_batch)/save_time:.1f} docs/sec\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to save batch {self.batch_count} to GCS: {e}\")\n",
        "            # Clean up temp file if it exists\n",
        "            try:\n",
        "                if 'temp_file_path' in locals():\n",
        "                    os.unlink(temp_file_path)\n",
        "            except:\n",
        "                pass\n",
        "            raise\n",
        "        \n",
        "        # Save progress after each batch\n",
        "        self._save_progress()\n",
        "        \n",
        "        # Clear current batch to free memory\n",
        "        self.current_batch = []\n",
        "    \n",
        "    def finalize(self):\n",
        "        \"\"\"\n",
        "        Save any remaining documents and return statistics.\n",
        "        \"\"\"\n",
        "        finalize_start = time.time()\n",
        "        \n",
        "        # Save remaining documents if any\n",
        "        if self.current_batch:\n",
        "            print(f\"🔄 Finalizing: saving remaining {len(self.current_batch)} documents to GCS...\")\n",
        "            self._save_current_batch()\n",
        "        \n",
        "        finalize_time = time.time() - finalize_start\n",
        "        print(f\"✅ GCS finalization completed in {finalize_time:.3f}s\")\n",
        "        print(f\"📊 CloudSaveDriver (GCS) completed:\")\n",
        "        print(f\"  - Total batches: {self.batch_count}\")\n",
        "        print(f\"  - Total documents: {self.documents_processed}\")\n",
        "        print(f\"  - Bucket: gs://{self.bucket_name}\")\n",
        "        \n",
        "        # Clean up progress file on successful completion\n",
        "        # if os.path.exists(self.progress_file):\n",
        "        #     os.remove(self.progress_file)\n",
        "        #     print(\"🧹 Cleaned up progress file\")\n",
        "        \n",
        "        return self.batch_count, self.documents_processed\n",
        "    \n",
        "    def get_statistics(self):\n",
        "        \"\"\"Get current statistics.\"\"\"\n",
        "        return {\n",
        "            'batches_created': self.batch_count,\n",
        "            'documents_processed': self.documents_processed,\n",
        "            'current_batch_size': len(self.current_batch),\n",
        "            'storage_type': 'gcs',\n",
        "            'bucket_name': self.bucket_name,\n",
        "            'project_id': self.project_id\n",
        "        }\n",
        "    \n",
        "    def list_batches(self):\n",
        "        \"\"\"\n",
        "        List all batch files in the GCS bucket.\n",
        "        \n",
        "        Returns:\n",
        "            list: List of blob objects representing batch files\n",
        "        \"\"\"\n",
        "        try:\n",
        "            blobs = list(self.bucket.list_blobs(prefix=\"batch_\"))\n",
        "            return sorted(blobs, key=lambda x: x.name)\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to list batches from GCS: {e}\")\n",
        "            return []\n",
        "    \n",
        "    def load_batch(self, blob):\n",
        "        \"\"\"\n",
        "        Load a batch from GCS.\n",
        "        \n",
        "        Args:\n",
        "            blob: GCS blob object or blob name\n",
        "            \n",
        "        Returns:\n",
        "            list: List of processed documents\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if isinstance(blob, str):\n",
        "                blob = self.bucket.blob(blob)\n",
        "            \n",
        "            # Download to temporary file\n",
        "            with tempfile.NamedTemporaryFile(mode='w+', suffix='.json', delete=False) as temp_file:\n",
        "                blob.download_to_filename(temp_file.name)\n",
        "                \n",
        "                # Load JSON data\n",
        "                with open(temp_file.name, 'r', encoding='utf-8') as f:\n",
        "                    data = json.load(f)\n",
        "                \n",
        "                # Clean up\n",
        "                import os\n",
        "                os.unlink(temp_file.name)\n",
        "                \n",
        "                return data\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to load batch from GCS: {e}\")\n",
        "            return []\n",
        "class CloudParquetSaveDriver(CloudSaveDriver):\n",
        "    \"\"\"\n",
        "    Google Cloud Storage (GCS) implementation for saving processed batches to parquet files.\n",
        "    \"\"\"\n",
        "    def __init__(self, bucket_name=None, project_id=None, batch_size=100, progress_file=\"gcs_processing_progress.json\"):\n",
        "        super().__init__(bucket_name, project_id, batch_size, progress_file)\n",
        "\n",
        "    def _save_current_batch(self):\n",
        "        \"\"\"\n",
        "        Save the current batch to GCS bucket.\n",
        "        \"\"\"\n",
        "        if not self.current_batch:\n",
        "            return\n",
        "\n",
        "        self.batch_count += 1\n",
        "\n",
        "        # Create filename with timestamp and batch number\n",
        "        timestamp = int(time.time())\n",
        "        filename = f\"batch_{self.batch_count:06d}_{timestamp}.parquet\"\n",
        "        save_start = time.time()\n",
        "        try:\n",
        "            # Create temporary file for Parquet data\n",
        "            with tempfile.NamedTemporaryFile(mode='w', suffix='.parquet', delete=False) as temp_file:\n",
        "                pa.Table.from_pylist(self.current_batch).to_pandas().to_parquet(temp_file.name)\n",
        "                temp_file_path = temp_file.name\n",
        "\n",
        "                blob = self.bucket.blob(filename)\n",
        "\n",
        "                for attempt in range(self.GCS_RETRY_ATTEMPTS):\n",
        "                    try:\n",
        "                        blob.upload_from_filename(temp_file_path, content_type='application/parquet')\n",
        "\n",
        "                        break\n",
        "                    except self.GoogleCloudError as e:\n",
        "                        if attempt == self.GCS_RETRY_ATTEMPTS - 1:\n",
        "                            raise\n",
        "                        print(f\"⚠️  Upload attempt {attempt + 1} failed, retrying... Error: {e}\")\n",
        "                        time.sleep(2 ** attempt)  # Exponential backoff\n",
        "\n",
        "                os.unlink(temp_file_path)  # Clean up\n",
        "\n",
        "                save_time = time.time() - save_start\n",
        "                file_size_mb = len(json.dumps(self.current_batch)) / 1024 / 1024\n",
        "\n",
        "                print(f\"☁️  Saved batch {self.batch_count} with {len(self.current_batch)} documents to gs://{self.bucket_name}/{filename}\")\n",
        "                print(f\"   ⏱️  Upload time: {save_time:.3f}s, Size: {file_size_mb:.1f} MB, Rate: {len(self.current_batch)/save_time:.1f} docs/sec\")\n",
        "\n",
        "                self._save_progress()\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to save batch {self.batch_count} to GCS: {e}\")\n",
        "            # Clean up temp file if it exists\n",
        "            try:\n",
        "                if 'temp_file_path' in locals():\n",
        "                    os.unlink(temp_file_path)\n",
        "            except:\n",
        "                pass\n",
        "            raise\n",
        "\n",
        "\n",
        "    def load_batch(self, blob):\n",
        "        \"\"\"\n",
        "        Load a batch from GCS.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if isinstance(blob, str):\n",
        "                blob = self.bucket.blob(blob)\n",
        "            else:\n",
        "                # Download to temporary file\n",
        "                with tempfile.NamedTemporaryFile(mode='w+', suffix='.parquet', delete=False) as temp_file:\n",
        "                    blob.download_to_filename(temp_file.name)\n",
        "\n",
        "                    # Load Parquet data\n",
        "                    table = pq.read_table(temp_file.name)\n",
        "\n",
        "                    # Clean up\n",
        "                    os.unlink(temp_file.name)\n",
        "\n",
        "                    return table.to_pylist()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to load batch from GCS: {e}\")\n",
        "            return []\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ojy7lkFrrXDd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ojy7lkFrrXDd",
        "outputId": "c97e64f8-9d3d-42e0-d931-a811ddb1f504"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-trf==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.8.0/en_core_web_trf-3.8.0-py3-none-any.whl (457.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.4/457.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-curated-transformers<1.0.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from en-core-web-trf==3.8.0) (0.3.1)\n",
            "Requirement already satisfied: curated-transformers<0.2.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (0.1.1)\n",
            "Requirement already satisfied: curated-tokenizers<0.1.0,>=0.0.9 in /usr/local/lib/python3.12/dist-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (0.0.9)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2.8.0+cu126)\n",
            "Requirement already satisfied: regex>=2022 in /usr/local/lib/python3.12/dist-packages (from curated-tokenizers<0.1.0,>=0.0.9->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2024.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.0.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_trf')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_trf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3xuFwv0utVM4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xuFwv0utVM4",
        "outputId": "2eacc77e-456c-45db-a507-12bbb2cb45eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets==3.6.0 in /usr/local/lib/python3.12/dist-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (0.35.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets==3.6.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets==3.6.0) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==3.6.0) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==3.6.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==3.6.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==3.6.0) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==3.6.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==3.6.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==3.6.0) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.6.0) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets==3.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LcCHxYzFq6eF",
      "metadata": {
        "id": "LcCHxYzFq6eF"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af15f6fe",
      "metadata": {
        "id": "af15f6fe"
      },
      "outputs": [],
      "source": [
        "generator = SpacyJSONGenerator(batch_size=50, n_process=1,require_gpu=True)\n",
        "dataset = load_dataset(\"Skylion007/openwebtext\", trust_remote_code=True, streaming=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9df0457",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9df0457",
        "outputId": "f2153dfb-edd6-403e-d998-2756f8657acf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📋 Loaded existing progress: 0 docs, 0 batches\n",
            "☁️  CloudSaveDriver (GCS) initialized:\n",
            "  - Bucket: parquet_v1_openwebtext-with-pos-ner\n",
            "  - Project: eastern-bridge-472408-d3\n",
            "  - Batch size: 500\n"
          ]
        }
      ],
      "source": [
        "gcs_save_driver = CloudParquetSaveDriver(\n",
        "    bucket_name=\"parquet_v1_openwebtext-with-pos-ner\",\n",
        "    project_id=\"eastern-bridge-472408-d3\",\n",
        "    batch_size=500  # Small batch size for testing\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DiTmidF2tAA0",
      "metadata": {
        "id": "DiTmidF2tAA0"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "766de082",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "766de082",
        "outputId": "df9c8b57-d318-49f4-d56e-aaf089a12d90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Starting HF map() optimized processing with CloudParquetSaveDriver...\n",
            "Skipping documents (if needed) and adding mapping\n",
            "💾 Processing and saving data...\n",
            "❌ Error processing batch: CUDA out of memory. Tried to allocate 366.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 172.12 MiB is free. Process 20133 has 14.57 GiB memory in use. Of the allocated memory 13.08 GiB is allocated by PyTorch, and 1.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "❌ Error processing batch: CUDA out of memory. Tried to allocate 366.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 172.12 MiB is free. Process 20133 has 14.57 GiB memory in use. Of the allocated memory 13.08 GiB is allocated by PyTorch, and 1.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "⚠️  Processing interrupted by user. Progress saved.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3408607295.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m result_driver = generator.process_and_save(\n\u001b[0m\u001b[1;32m      2\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0msave_driver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgcs_save_driver\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mnum_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m  \u001b[0;31m# Process the whole dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         )\n",
            "\u001b[0;32m/tmp/ipython-input-40703895.py\u001b[0m in \u001b[0;36mprocess_and_save\u001b[0;34m(self, dataset, save_driver, num_batches, resume_from_progress)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessed_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2268\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2270\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex_iterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2271\u001b[0m             \u001b[0;31m# no need to format thanks to FormattedExamplesIterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2272\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1082\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\u001b[0m in \u001b[0;36m_iter\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1261\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtransformed_example\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_batch_to_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m                 )\n\u001b[0;32m-> 1263\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_example\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1264\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"previous_state\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_examples_since_previous_state\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1258\u001b[0m                 outputs = (\n\u001b[1;32m   1259\u001b[0m                     \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_example\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtransformed_example\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_batch_to_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\u001b[0m in \u001b[0;36miter_outputs\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1246\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatched\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1247\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"previous_state_example_idx\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1248\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_example\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1249\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatched\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\u001b[0m in \u001b[0;36mapply_function\u001b[0;34m(key_example, indices)\u001b[0m\n\u001b[1;32m   1176\u001b[0m             \u001b[0;34m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditional_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_example\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mprocessed_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0madditional_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mprepare_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_example\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-40703895.py\u001b[0m in \u001b[0;36mprocess_batch_texts\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m                 \u001b[0mprocessed_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_sentences_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'processed'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprocessed_docs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-40703895.py\u001b[0m in \u001b[0;36mprocess_sentences_batch\u001b[0;34m(self, sentences, sentence_ids)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;31m# Process batch with spaCy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_process\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_process\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;31m# Process each doc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36mpipe\u001b[0;34m(self, texts, as_tuples, batch_size, disable, component_cfg, n_process)\u001b[0m\n\u001b[1;32m   1620\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mpipe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpipes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1621\u001b[0m                 \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1622\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1623\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36m_pipe\u001b[0;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[1;32m   1712\u001b[0m ) -> Iterator[\"Doc\"]:\n\u001b[1;32m   1713\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pipe\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1714\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1715\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1716\u001b[0m         \u001b[0;31m# We added some args for pipe that __call__ doesn't expect.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy/pipeline/transition_parser.pyx\u001b[0m in \u001b[0;36mpipe\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mminibatch\u001b[0;34m(items, size)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1662\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36m_pipe\u001b[0;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[1;32m   1712\u001b[0m ) -> Iterator[\"Doc\"]:\n\u001b[1;32m   1713\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pipe\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1714\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1715\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1716\u001b[0m         \u001b[0;31m# We added some args for pipe that __call__ doesn't expect.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy/pipeline/pipe.pyx\u001b[0m in \u001b[0;36mpipe\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36m_pipe\u001b[0;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[1;32m   1712\u001b[0m ) -> Iterator[\"Doc\"]:\n\u001b[1;32m   1713\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pipe\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1714\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1715\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1716\u001b[0m         \u001b[0;31m# We added some args for pipe that __call__ doesn't expect.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy/pipeline/pipe.pyx\u001b[0m in \u001b[0;36mpipe\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36m_pipe\u001b[0;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[1;32m   1712\u001b[0m ) -> Iterator[\"Doc\"]:\n\u001b[1;32m   1713\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pipe\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1714\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1715\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1716\u001b[0m         \u001b[0;31m# We added some args for pipe that __call__ doesn't expect.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy/pipeline/transition_parser.pyx\u001b[0m in \u001b[0;36mpipe\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mminibatch\u001b[0;34m(items, size)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1662\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36m_pipe\u001b[0;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[1;32m   1712\u001b[0m ) -> Iterator[\"Doc\"]:\n\u001b[1;32m   1713\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pipe\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1714\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1715\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1716\u001b[0m         \u001b[0;31m# We added some args for pipe that __call__ doesn't expect.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy/pipeline/trainable_pipe.pyx\u001b[0m in \u001b[0;36mpipe\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mminibatch\u001b[0;34m(items, size)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1662\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36m_pipe\u001b[0;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[1;32m   1712\u001b[0m ) -> Iterator[\"Doc\"]:\n\u001b[1;32m   1713\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pipe\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1714\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1715\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1716\u001b[0m         \u001b[0;31m# We added some args for pipe that __call__ doesn't expect.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy_curated_transformers/pipeline/transformer.py\u001b[0m in \u001b[0;36mpipe\u001b[0;34m(self, stream, batch_size)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0m_install_extensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_annotations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy_curated_transformers/pipeline/transformer.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;31m# To ensure that the model's internal state is always consistent with the pipe's.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_model_all_layer_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_layer_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     def set_annotations(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/thinc/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0monly\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstead\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \"\"\"\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfinish_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy_curated_transformers/models/architectures.py\u001b[0m in \u001b[0;36mtransformer_model_forward\u001b[0;34m(model, docs, is_train)\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTransformerModelT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTransformerInT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m ) -> Tuple[TransformerOutT, TransformerBackpropT]:\n\u001b[0;32m--> 651\u001b[0;31m     \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackprop_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    309\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy_curated_transformers/models/with_non_ws_tokens.py\u001b[0m in \u001b[0;36mwith_non_ws_tokens_forward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;31m# here however, so that the models trained previously still load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m# correctly. We need to remove this in future when we retrain.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackprop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    309\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/thinc/layers/chain.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    309\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy_curated_transformers/tokenization/bbpe_encoder.py\u001b[0m in \u001b[0;36mbyte_bpe_encoder_forward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                 \u001b[0mpiece_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbbp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_as_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mdoc_pieces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpiece_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "result_driver = generator.process_and_save(\n",
        "            dataset=dataset,\n",
        "            save_driver=gcs_save_driver,\n",
        "            num_batches=3  # Process the whole dataset\n",
        "        )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "deep",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
