{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yGit741/nlp-3523-final-project/blob/data-prep/src/data_prep/data-prep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9af1d2cd",
      "metadata": {
        "id": "9af1d2cd"
      },
      "source": [
        "Data preparation pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7916723",
      "metadata": {
        "id": "b7916723",
        "outputId": "6d6803ca-ffde-4ca1-ade1-74cd1c9687d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%reload_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "class BaseSaveDriver(ABC):\n",
        "    \"\"\"\n",
        "    Abstract base class for save drivers.\n",
        "    Allows different storage implementations (local, cloud, etc.).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, batch_size: int = 100):\n",
        "        \"\"\"\n",
        "        Initialize the base save driver.\n",
        "\n",
        "        Args:\n",
        "            batch_size: Number of documents per batch\n",
        "        \"\"\"\n",
        "        self.batch_size = batch_size\n",
        "        self.current_batch = []\n",
        "        self.batch_count = 0\n",
        "        self.documents_processed = 0\n",
        "\n",
        "    @abstractmethod\n",
        "    def add_document(self, document):\n",
        "        \"\"\"Add a document to the current batch.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def finalize(self):\n",
        "        \"\"\"Save any remaining documents and return statistics.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_statistics(self):\n",
        "        \"\"\"Get current statistics.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def _save_current_batch(self):\n",
        "        \"\"\"Abstract method to save the current batch to storage.\"\"\"\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "E_he16XxpWQO"
      },
      "id": "E_he16XxpWQO",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c72c63d3",
      "metadata": {
        "id": "c72c63d3"
      },
      "outputs": [],
      "source": [
        "from spacy.pipeline import EntityRuler\n",
        "import spacy\n",
        "import re\n",
        "from typing import List, Dict, Any, Iterator\n",
        "\n",
        "class SpacyJSONGenerator:\n",
        "    def __init__(self, batch_size: int = 100, n_process: int = 1, require_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Initialize the generator with batching capabilities.\n",
        "\n",
        "        Args:\n",
        "            batch_size: Number of texts (sentences) to process in each batch\n",
        "            n_process: Number of processes for parallel processing (use -1 for all cores)\n",
        "        \"\"\"\n",
        "        # Load the transformer model\n",
        "        if require_gpu:\n",
        "            spacy.require_gpu()\n",
        "        self.nlp = spacy.load(\"en_core_web_trf\", disable=[\"lemmatizer\"])\n",
        "        self.batch_size = batch_size\n",
        "        self.n_process = n_process\n",
        "\n",
        "        # Add EntityRuler for NLE extraction (BEFORE NER for better integration)\n",
        "        ruler = self.nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
        "        self._setup_nle_patterns(ruler)\n",
        "\n",
        "    def _setup_nle_patterns(self, ruler: EntityRuler):\n",
        "        \"\"\"Setup patterns for Nonlinguistic Entity extraction using EntityRuler.\"\"\"\n",
        "        patterns = [\n",
        "            # Phone patterns\n",
        "            {\"label\": \"PHONE\", \"pattern\": [{\"TEXT\": {\"REGEX\": r\"\\(?[0-9]{3}\\)?[-.\\s]?[0-9]{3}[-.\\s]?[0-9]{4}\"}}]},\n",
        "            {\"label\": \"PHONE\", \"pattern\": [{\"TEXT\": {\"REGEX\": r\"\\+[1-9]\\d{1,14}\"}}]},\n",
        "\n",
        "            # Address patterns\n",
        "            {\"label\": \"ADDRESS\", \"pattern\": [{\"IS_DIGIT\": True}, {\"IS_ALPHA\": True, \"OP\": \"+\"}, {\"LOWER\": {\"IN\": [\"st\", \"street\", \"ave\", \"avenue\", \"rd\", \"road\", \"blvd\", \"boulevard\", \"dr\", \"drive\", \"ln\", \"lane\", \"ct\", \"court\", \"pl\", \"place\"]}}]},\n",
        "            {\"label\": \"ADDRESS\", \"pattern\": [{\"LOWER\": \"p\"}, {\"TEXT\": \".\"}, {\"LOWER\": \"o\"}, {\"TEXT\": \".\"}, {\"LOWER\": \"box\"}, {\"IS_DIGIT\": True}]},\n",
        "\n",
        "            # IP Address patterns\n",
        "            {\"label\": \"IP_ADDRESS\", \"pattern\": [{\"TEXT\": {\"REGEX\": r\"\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b\"}}]},\n",
        "            {\"label\": \"IP_ADDRESS\", \"pattern\": [{\"TEXT\": {\"REGEX\": r\"\\b(?:[0-9a-fA-F]{1,4}:){7}[0-9a-fA-F]{1,4}\\b\"}}]},\n",
        "\n",
        "            # SSN patterns\n",
        "            {\"label\": \"SSN\", \"pattern\": [{\"IS_DIGIT\": True, \"LENGTH\": 3}, {\"TEXT\": \"-\"}, {\"IS_DIGIT\": True, \"LENGTH\": 2}, {\"TEXT\": \"-\"}, {\"IS_DIGIT\": True, \"LENGTH\": 4}]},\n",
        "            {\"label\": \"SSN\", \"pattern\": [{\"IS_DIGIT\": True, \"LENGTH\": 3}, {\"IS_SPACE\": True}, {\"IS_DIGIT\": True, \"LENGTH\": 2}, {\"IS_SPACE\": True}, {\"IS_DIGIT\": True, \"LENGTH\": 4}]},\n",
        "\n",
        "            # URL and Email patterns (using built-ins)\n",
        "            {\"label\": \"URL\", \"pattern\": [{\"LIKE_URL\": True}]},\n",
        "            {\"label\": \"EMAIL\", \"pattern\": [{\"LIKE_EMAIL\": True}]}\n",
        "        ]\n",
        "\n",
        "        ruler.add_patterns(patterns)\n",
        "\n",
        "\n",
        "    def _extract_punctuation_spans(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Extract punctuation spans from text.\"\"\"\n",
        "        punct_spans = []\n",
        "        punct_pattern = r'[^\\w\\s]'  # Match non-word, non-space characters\n",
        "\n",
        "        for match in re.finditer(punct_pattern, text):\n",
        "            punct_spans.append({\n",
        "                \"start\": match.start(),\n",
        "                \"end\": match.end(),\n",
        "                \"value\": match.group()\n",
        "            })\n",
        "\n",
        "        return punct_spans\n",
        "\n",
        "    def _extract_special_tags_from_doc(self, doc) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Extract special tags from spaCy doc (NLEs are now in doc.ents).\"\"\"\n",
        "        special_tags = []\n",
        "\n",
        "        # Filter NLE entities (non-standard NER labels)\n",
        "        nle_labels = {\"PHONE\", \"ADDRESS\", \"IP_ADDRESS\", \"SSN\", \"URL\", \"EMAIL\"}\n",
        "\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ in nle_labels:\n",
        "                special_tags.append({\n",
        "                    \"start\": ent.start_char,\n",
        "                    \"end\": ent.end_char,\n",
        "                    \"type\": ent.label_,\n",
        "                    \"value\": ent.text\n",
        "                })\n",
        "\n",
        "        return special_tags\n",
        "\n",
        "    def _get_sentence_spans(self, doc, text: str) -> List[Dict[str, int]]:\n",
        "        \"\"\"Extract sentence spans.\"\"\"\n",
        "        sent_spans = []\n",
        "        for sent in doc.sents:\n",
        "            sent_spans.append({\n",
        "                \"start\": sent.start_char,\n",
        "                \"end\": sent.end_char\n",
        "            })\n",
        "        return sent_spans\n",
        "\n",
        "    def process_single_doc(self, doc, original_text: str, sentence_id: str) -> Dict[str, Any]:\n",
        "        \"\"\"Process a single spaCy doc and return the JSON structure.\"\"\"\n",
        "\n",
        "        # Extract sentence spans\n",
        "        sent_spans = self._get_sentence_spans(doc, original_text)\n",
        "\n",
        "        # Extract punctuation spans\n",
        "        punct_spans = self._extract_punctuation_spans(original_text)\n",
        "\n",
        "        # Extract special tags (NLEs) from doc.ents\n",
        "        special_tags = self._extract_special_tags_from_doc(doc)\n",
        "\n",
        "        # Extract named entity spans with entity IDs (standard NER only)\n",
        "        ner_spans = []\n",
        "        nle_labels = {\"PHONE\", \"ADDRESS\", \"IP_ADDRESS\", \"SSN\", \"URL\", \"EMAIL\"}\n",
        "\n",
        "        for ent in doc.ents:\n",
        "            # Only include standard NER entities, not NLEs\n",
        "            if ent.label_ not in nle_labels:\n",
        "                ner_spans.append({\n",
        "                    \"entity_id\": f\"{ent.label_}-{str(ent).upper().replace(' ', '_').replace('-', '_')}\",\n",
        "                    \"start\": ent.start_char,\n",
        "                    \"end\": ent.end_char,\n",
        "                    \"label\": ent.label_\n",
        "                })\n",
        "\n",
        "        # Extract POS tokens and tags\n",
        "        pos_tokens = []\n",
        "        pos_tags = []\n",
        "        ner_iob = []\n",
        "\n",
        "        for token in doc:\n",
        "            # Skip whitespace-only tokens\n",
        "            if not token.text.strip():\n",
        "                continue\n",
        "\n",
        "            pos_tokens.append(token.text)\n",
        "            pos_tags.append(token.pos_)\n",
        "\n",
        "            # Determine IOB tag\n",
        "            if token.ent_iob_ == 'B':\n",
        "                ner_iob.append(f\"B-{token.ent_type_}\")\n",
        "            elif token.ent_iob_ == 'I':\n",
        "                ner_iob.append(f\"I-{token.ent_type_}\")\n",
        "            else:\n",
        "                ner_iob.append(\"O\")\n",
        "\n",
        "        # Build the final JSON structure\n",
        "        result = {\n",
        "            \"id\": sentence_id,\n",
        "            \"text\": original_text,\n",
        "            \"sent_spans\": sent_spans,\n",
        "            \"punct_spans\": punct_spans,\n",
        "            \"special_tags\": special_tags,\n",
        "            \"ner_spans\": ner_spans,\n",
        "            \"pos_tokens\": pos_tokens,\n",
        "            \"pos_tags\": pos_tags,\n",
        "            \"ner_iob\": ner_iob\n",
        "        }\n",
        "\n",
        "        return result\n",
        "\n",
        "    def process_sentences_batch(self, sentences: List[str], sentence_ids: List[str] = None):\n",
        "        \"\"\"Process a batch of sentences efficiently.\"\"\"\n",
        "        if sentence_ids is None:\n",
        "            sentence_ids = [f\"sent_{str(uuid.uuid4())}\" for _ in range(len(sentences))]\n",
        "\n",
        "        # Process batch with spaCy\n",
        "        docs = list(self.nlp.pipe(sentences, batch_size=self.batch_size, n_process=self.n_process))\n",
        "\n",
        "        # Process each doc\n",
        "        results = []\n",
        "        for doc, original_text, sent_id in zip(docs, sentences, sentence_ids):\n",
        "            result = self.process_single_doc(doc, original_text, sent_id)\n",
        "            results.append(result)\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "    def process_sentences_streaming(self, sentences: Iterator[str],\n",
        "                                   sentence_id_generator: Iterator[str] = None) -> Iterator[Dict[str, Any]]:\n",
        "        \"\"\"Process sentences in streaming fashion with batching.\"\"\"\n",
        "        sentence_batch = []\n",
        "        id_batch = []\n",
        "\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            sentence_batch.append(sentence)\n",
        "\n",
        "            if sentence_id_generator:\n",
        "                id_batch.append(next(sentence_id_generator))\n",
        "            else:\n",
        "                id_batch.append(f\"sent_{i:07d}\")\n",
        "\n",
        "            # Process batch when it reaches batch_size\n",
        "            if len(sentence_batch) >= self.batch_size:\n",
        "                results = self.process_sentences_batch(sentence_batch, id_batch)\n",
        "                for result in results:\n",
        "                    yield result\n",
        "\n",
        "                # Clear batches\n",
        "                sentence_batch = []\n",
        "                id_batch = []\n",
        "\n",
        "        # Process remaining sentences\n",
        "        if sentence_batch:\n",
        "            results = self.process_sentences_batch(sentence_batch)\n",
        "            for result in results:\n",
        "                yield result\n",
        "\n",
        "    def process_and_save(self, dataset, save_driver: BaseSaveDriver, num_batches=None, resume_from_progress=True):\n",
        "        \"\"\"\n",
        "        Process dataset using Hugging Face map() function with configurable save driver.\n",
        "        Includes detailed timing measurements and bottleneck analysis.\n",
        "\n",
        "        Args:\n",
        "            dataset: Hugging Face dataset\n",
        "            save_driver: SaveDriver instance for handling storage (local, cloud, etc.)\n",
        "            num_batches: Number of batches to process (None = process all)\n",
        "            resume_from_progress: Whether to resume from existing progress (if available)\n",
        "\n",
        "        Returns:\n",
        "            BaseSaveDriver: The save driver instance with statistics\n",
        "        \"\"\"\n",
        "\n",
        "        print(f\"🚀 Starting HF map() optimized processing with {save_driver.__class__.__name__}...\")\n",
        "\n",
        "        # Check if we should resume from existing progress\n",
        "        documents_to_skip = 0\n",
        "        initial_batch_count = 0\n",
        "        if resume_from_progress and hasattr(save_driver, 'progress_data'):\n",
        "            progress = save_driver.progress_data\n",
        "            if progress['documents_processed'] > 0:\n",
        "                documents_to_skip = progress['documents_processed']\n",
        "                initial_batch_count = progress['batch_count']\n",
        "\n",
        "                print(f\"🔄 Resuming from previous progress:\")\n",
        "                print(f\"   📄 Documents already processed: {progress['documents_processed']}\")\n",
        "                print(f\"   📦 Batches already created: {progress['batch_count']}\")\n",
        "                print(f\"⏭️  Skipping first {documents_to_skip} documents...\")\n",
        "\n",
        "        def process_batch_texts(batch):\n",
        "            \"\"\"Process a batch of texts with spaCy using HF map.\"\"\"\n",
        "\n",
        "            texts = [text for text in batch['text'] if len(text) >= 10]\n",
        "\n",
        "            if not texts:\n",
        "                return {'processed': [None] * len(batch['text'])}\n",
        "\n",
        "            try:\n",
        "                processed_docs = self.process_sentences_batch(texts)\n",
        "                return {'processed': processed_docs}\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error processing batch: {e}\")\n",
        "                return {'processed': [None] * len(batch['text'])}\n",
        "\n",
        "        print(f\"Skipping documents (if needed) and adding mapping\")\n",
        "        processed_dataset = dataset['train'].skip(documents_to_skip * self.batch_size).map(\n",
        "            process_batch_texts,\n",
        "            batched=True,\n",
        "            batch_size=self.batch_size,\n",
        "            remove_columns=['text']\n",
        "        )\n",
        "\n",
        "        print(\"💾 Processing and saving data...\")\n",
        "\n",
        "        processed_count = 0\n",
        "\n",
        "        try:\n",
        "            for example in processed_dataset:\n",
        "\n",
        "\n",
        "                save_driver.add_document(example['processed'])\n",
        "                processed_count += 1\n",
        "\n",
        "                # Check batch count more frequently to respect num_batches limit\n",
        "                current_batch_count = save_driver.batch_count\n",
        "                new_batches_created = current_batch_count - initial_batch_count\n",
        "\n",
        "                # Check if we've processed enough NEW batches (check after each document)\n",
        "                if num_batches is not None and new_batches_created >= num_batches:\n",
        "                    print(f\"🛑 Reached target of {num_batches} new batches. Stopping...\")\n",
        "                    print(f\"   📊 Total batches: {current_batch_count}, New batches this run: {new_batches_created}\")\n",
        "                    break\n",
        "\n",
        "\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n⚠️  Processing interrupted by user. Progress saved.\")\n",
        "            if hasattr(save_driver, '_save_progress'):\n",
        "                save_driver._save_progress()\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            print(f\"\\n❌ Processing failed: {e}\")\n",
        "            print(\"💾 Progress saved. You can resume later.\")\n",
        "            if hasattr(save_driver, '_save_progress'):\n",
        "                save_driver._save_progress()\n",
        "            raise\n",
        "\n",
        "\n",
        "        # Finalize and get statistics\n",
        "        batch_count, documents_processed = save_driver.finalize()\n",
        "\n",
        "        # Calculate total time and performance metrics\n",
        "\n",
        "        print(f\"\\n🎉 Processing completed!\")\n",
        "        print(f\"📊 Performance Summary:\")\n",
        "        print(f\"   📄 Documents processed: {documents_processed}\")\n",
        "        print(f\"   📦 Batches created: {batch_count}\")\n",
        "\n",
        "        return save_driver\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6T6xD4HhoBvk"
      },
      "id": "6T6xD4HhoBvk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af15f6fe",
      "metadata": {
        "id": "af15f6fe"
      },
      "outputs": [],
      "source": [
        "generator = SpacyJSONGenerator(batch_size=50, n_process=1,require_gpu=True)\n",
        "dataset = load_dataset(\"Skylion007/openwebtext\", trust_remote_code=True, streaming=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9df0457",
      "metadata": {
        "id": "e9df0457",
        "outputId": "c31a34a4-4bb1-4d9f-afb3-a6012ac95aa2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "☁️  CloudSaveDriver (GCS) initialized:\n",
            "  - Bucket: parquet_v1_openwebtext-with-pos-ner\n",
            "  - Project: eastern-bridge-472408-d3\n",
            "  - Batch size: 50\n"
          ]
        }
      ],
      "source": [
        "gcs_save_driver = CloudParquetSaveDriver(\n",
        "    bucket_name=Config.GCS_BUCKET_NAME,\n",
        "    project_id=Config.GCS_PROJECT_ID,\n",
        "    batch_size=50  # Small batch size for testing\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "766de082",
      "metadata": {
        "id": "766de082",
        "outputId": "c108ebef-59f6-40ff-9c50-d2acb18fd866"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Starting HF map() optimized processing with CloudParquetSaveDriver...\n",
            "Skipping documents (if needed) and adding mapping\n",
            "💾 Processing and saving data...\n",
            "☁️  Saved batch 1 with 50 documents to gs://parquet_v1_openwebtext-with-pos-ner/batch_000001_1759061758.parquet\n",
            "   ⏱️  Upload time: 2.192s, Size: 2.5 MB, Rate: 22.8 docs/sec\n",
            "🛑 Reached target of 1 new batches. Stopping...\n",
            "   📊 Total batches: 1, New batches this run: 1\n",
            "🔄 Finalizing: saving remaining 50 documents to GCS...\n",
            "☁️  Saved batch 2 with 50 documents to gs://parquet_v1_openwebtext-with-pos-ner/batch_000002_1759061760.parquet\n",
            "   ⏱️  Upload time: 1.869s, Size: 2.5 MB, Rate: 26.7 docs/sec\n",
            "✅ GCS finalization completed in 1.897s\n",
            "📊 CloudSaveDriver (GCS) completed:\n",
            "  - Total batches: 2\n",
            "  - Total documents: 50\n",
            "  - Bucket: gs://parquet_v1_openwebtext-with-pos-ner\n",
            "\n",
            "🎉 Processing completed!\n",
            "📊 Performance Summary:\n",
            "   📄 Documents processed: 50\n",
            "   📦 Batches created: 2\n"
          ]
        }
      ],
      "source": [
        "result_driver = generator.process_and_save(\n",
        "            dataset=dataset,\n",
        "            save_driver=gcs_save_driver,\n",
        "            num_batches=1  # Process the whole dataset\n",
        "        )"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "deep",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}