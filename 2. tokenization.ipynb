{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tokenization and Masking\n",
        "\n",
        "Comprehensive tokenization module for Knowledge vs Reasoning Separation project.\n",
        "\n",
        "This notebook consolidates all tokenization functionality including:\n",
        "- **Enhanced Tokenizer** with structural hints preservation\n",
        "- **Îµ-masking** with NER entity preservation\n",
        "- **GCS Data Interface** for loading and processing large datasets\n",
        "- **Tokenization Analysis** and visualization tools\n",
        "- **Performance benchmarking** and optimization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "from typing import List, Dict, Optional, Union, Tuple, Any, Set\n",
        "from pathlib import Path\n",
        "from collections import defaultdict, Counter\n",
        "import json\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import re\n",
        "import warnings\n",
        "import tempfile\n",
        "import os\n",
        "import gc\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# GCS and data processing imports\n",
        "try:\n",
        "    import gcsfs\n",
        "    import pyarrow.parquet as pq\n",
        "    import polars as pl\n",
        "    GCS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    GCS_AVAILABLE = False\n",
        "    print(\"Warning: GCS dependencies not available. Install with: pip install gcsfs pyarrow polars\")\n",
        "\n",
        "# NLP imports\n",
        "try:\n",
        "    import spacy\n",
        "    SPACY_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SPACY_AVAILABLE = False\n",
        "    print(\"Warning: spaCy not available. Install with: pip install spacy && python -m spacy download en_core_web_sm\")\n",
        "\n",
        "print(\"All imports successful!\")\n",
        "print(f\"GCS support: {'âœ…' if GCS_AVAILABLE else 'âŒ'}\")\n",
        "print(f\"spaCy support: {'âœ…' if SPACY_AVAILABLE else 'âŒ'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## 2. GCS Data Interface\n",
        "\n",
        "Google Cloud Storage interface for loading and processing large datasets, matching the API style from masking.py.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GCSDataLoader:\n",
        "    \"\"\"\n",
        "    Google Cloud Storage data loader with interface matching masking.py API.\n",
        "    \n",
        "    Features:\n",
        "    - Load parquet files from GCS buckets\n",
        "    - Batch processing with progress tracking\n",
        "    - Memory-efficient streaming\n",
        "    - Error handling and retry logic\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, bucket_name: str, credentials_path: str = \"eastern-bridge-credentials.json\"):\n",
        "        \"\"\"\n",
        "        Initialize GCS data loader.\n",
        "        \n",
        "        Args:\n",
        "            bucket_name: GCS bucket name\n",
        "            credentials_path: Path to service account credentials\n",
        "        \"\"\"\n",
        "        if not GCS_AVAILABLE:\n",
        "            raise ImportError(\"GCS dependencies not available. Install with: pip install gcsfs pyarrow polars\")\n",
        "        \n",
        "        self.bucket_name = bucket_name\n",
        "        self.credentials_path = credentials_path\n",
        "        \n",
        "        # Initialize GCS filesystem\n",
        "        try:\n",
        "            self.fs = gcsfs.GCSFileSystem(token=credentials_path)\n",
        "            print(f\"âœ… GCS connection established to bucket: {bucket_name}\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to connect to GCS: {e}\")\n",
        "        \n",
        "        # Cache for file listings\n",
        "        self._file_cache = None\n",
        "        self._cache_timestamp = None\n",
        "    \n",
        "    def list_files(self, pattern: str = \"**/*.parquet\", refresh: bool = False) -> List[str]:\n",
        "        \"\"\"\n",
        "        List files in the bucket matching the pattern.\n",
        "        \n",
        "        Args:\n",
        "            pattern: Glob pattern for file matching\n",
        "            refresh: Force refresh of file cache\n",
        "            \n",
        "        Returns:\n",
        "            List of file paths\n",
        "        \"\"\"\n",
        "        cache_valid = (self._file_cache is not None and \n",
        "                      self._cache_timestamp is not None and \n",
        "                      time.time() - self._cache_timestamp < 300)  # 5 min cache\n",
        "        \n",
        "        if refresh or not cache_valid:\n",
        "            try:\n",
        "                files = sorted(self.fs.glob(f\"{self.bucket_name}/{pattern}\"))\n",
        "                self._file_cache = files\n",
        "                self._cache_timestamp = time.time()\n",
        "                print(f\"ðŸ“ Found {len(files)} files matching pattern: {pattern}\")\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Failed to list files: {e}\")\n",
        "                return []\n",
        "        \n",
        "        return self._file_cache\n",
        "    \n",
        "    def load_parquet_file(self, file_path: str) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Load a single parquet file from GCS.\n",
        "        \n",
        "        Args:\n",
        "            file_path: Path to parquet file\n",
        "            \n",
        "        Returns:\n",
        "            DataFrame or None if failed\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Use polars for efficient loading\n",
        "            df = pl.read_parquet(f\"gs://{file_path}\").to_pandas()\n",
        "            print(f\"ðŸ“Š Loaded {len(df)} rows from {file_path}\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Failed to load {file_path}: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def load_batch_files(self, file_paths: List[str], max_files: Optional[int] = None) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Load multiple parquet files and combine into single DataFrame.\n",
        "        \n",
        "        Args:\n",
        "            file_paths: List of file paths to load\n",
        "            max_files: Maximum number of files to load (None for all)\n",
        "            \n",
        "        Returns:\n",
        "            Combined DataFrame\n",
        "        \"\"\"\n",
        "        if max_files:\n",
        "            file_paths = file_paths[:max_files]\n",
        "        \n",
        "        print(f\"ðŸ”„ Loading {len(file_paths)} files...\")\n",
        "        dfs = []\n",
        "        \n",
        "        for i, file_path in enumerate(file_paths):\n",
        "            df = self.load_parquet_file(file_path)\n",
        "            if df is not None:\n",
        "                dfs.append(df)\n",
        "            \n",
        "            if (i + 1) % 10 == 0:\n",
        "                print(f\"   Processed {i + 1}/{len(file_paths)} files\")\n",
        "        \n",
        "        if dfs:\n",
        "            combined_df = pd.concat(dfs, ignore_index=True)\n",
        "            print(f\"âœ… Combined {len(dfs)} files into {len(combined_df)} total rows\")\n",
        "            return combined_df\n",
        "        else:\n",
        "            print(\"âŒ No files loaded successfully\")\n",
        "            return pd.DataFrame()\n",
        "    \n",
        "    def stream_files(self, file_paths: List[str], batch_size: int = 1000):\n",
        "        \"\"\"\n",
        "        Stream files in batches for memory-efficient processing.\n",
        "        \n",
        "        Args:\n",
        "            file_paths: List of file paths to process\n",
        "            batch_size: Number of rows per batch\n",
        "            \n",
        "        Yields:\n",
        "            DataFrame batches\n",
        "        \"\"\"\n",
        "        for file_path in file_paths:\n",
        "            df = self.load_parquet_file(file_path)\n",
        "            if df is not None:\n",
        "                # Yield in batches\n",
        "                for i in range(0, len(df), batch_size):\n",
        "                    batch = df.iloc[i:i + batch_size]\n",
        "                    yield batch\n",
        "    \n",
        "    def get_file_info(self, file_paths: Optional[List[str]] = None) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Get information about files in the bucket.\n",
        "        \n",
        "        Args:\n",
        "            file_paths: Specific files to analyze (None for all)\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary with file statistics\n",
        "        \"\"\"\n",
        "        if file_paths is None:\n",
        "            file_paths = self.list_files()\n",
        "        \n",
        "        info = {\n",
        "            \"total_files\": len(file_paths),\n",
        "            \"total_size_mb\": 0,\n",
        "            \"file_sizes\": [],\n",
        "            \"sample_files\": file_paths[:5] if file_paths else []\n",
        "        }\n",
        "        \n",
        "        for file_path in file_paths[:10]:  # Sample first 10 files\n",
        "            try:\n",
        "                stat = self.fs.stat(file_path)\n",
        "                size_mb = stat['size'] / (1024 * 1024)\n",
        "                info[\"file_sizes\"].append(size_mb)\n",
        "                info[\"total_size_mb\"] += size_mb\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not get size for {file_path}: {e}\")\n",
        "        \n",
        "        if info[\"file_sizes\"]:\n",
        "            info[\"avg_file_size_mb\"] = np.mean(info[\"file_sizes\"])\n",
        "            info[\"max_file_size_mb\"] = np.max(info[\"file_sizes\"])\n",
        "            info[\"min_file_size_mb\"] = np.min(info[\"file_sizes\"])\n",
        "        \n",
        "        return info\n",
        "\n",
        "# Initialize GCS loader\n",
        "if GCS_AVAILABLE:\n",
        "    GCS_LOADER = GCSDataLoader(\"parquet_v2_openwebtext-with-pos-ner\")\n",
        "    print(\"âœ… GCS Data Loader initialized\")\n",
        "else:\n",
        "    GCS_LOADER = None\n",
        "    print(\"âŒ GCS Data Loader not available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Enhanced Tokenizer with Structural Hints\n",
        "\n",
        "Implementation of the enhanced tokenizer that preserves structural hints during Îµ-masking.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NERExtractor:\n",
        "    \"\"\"\n",
        "    Pluggable NER extraction supporting spaCy and regex fallback.\n",
        "    Extracts named entities to preserve as structural hints during masking.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, method: str = \"auto\"):\n",
        "        \"\"\"\n",
        "        Initialize NER extractor.\n",
        "        \n",
        "        Args:\n",
        "            method: \"spacy\", \"regex\", or \"auto\" (try spaCy, fall back to regex)\n",
        "        \"\"\"\n",
        "        self.method = method\n",
        "        self.nlp = None\n",
        "        self.entity_counter = 0\n",
        "        \n",
        "        if method in (\"spacy\", \"auto\") and SPACY_AVAILABLE:\n",
        "            try:\n",
        "                self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "                self.method = \"spacy\"\n",
        "                print(\"âœ… spaCy NER model loaded\")\n",
        "            except (ImportError, OSError):\n",
        "                if method == \"spacy\":\n",
        "                    raise RuntimeError(\"spaCy not available. Install with: pip install spacy && python -m spacy download en_core_web_sm\")\n",
        "                self.method = \"regex\"\n",
        "                print(\"âš ï¸ Falling back to regex NER\")\n",
        "        else:\n",
        "            self.method = \"regex\"\n",
        "            print(\"âš ï¸ Using regex NER (spaCy not available)\")\n",
        "        \n",
        "        # Regex patterns for common entity types (fallback)\n",
        "        self._regex_patterns = {\n",
        "            \"PERSON\": re.compile(r'\\b[A-Z][a-z]+ [A-Z][a-z]+\\b'),\n",
        "            \"ORG\": re.compile(r'\\b(?:Inc\\.|Corp\\.|Ltd\\.|Company|Corporation|University|Institute)\\b'),\n",
        "            \"GPE\": re.compile(r'\\b(?:Paris|London|New York|Tokyo|Berlin|Rome|Madrid|Washington|Boston|Chicago)\\b'),\n",
        "            \"DATE\": re.compile(r'\\b\\d{4}\\b|\\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\\b'),\n",
        "            \"MONEY\": re.compile(r'\\$\\d+(?:\\.\\d{2})?\\b|\\b\\d+(?:\\.\\d{2})?\\s*(?:dollars?|USD|euros?|EUR)\\b'),\n",
        "        }\n",
        "    \n",
        "    def reset_counter(self):\n",
        "        \"\"\"Reset entity counter for new document.\"\"\"\n",
        "        self.entity_counter = 0\n",
        "    \n",
        "    def extract_entities(self, text: str) -> Dict[str, Tuple[str, str]]:\n",
        "        \"\"\"\n",
        "        Extract named entities from text.\n",
        "        \n",
        "        Args:\n",
        "            text: Input text\n",
        "            \n",
        "        Returns:\n",
        "            Dict mapping entity_text -> (entity_type, replacement_token)\n",
        "            Example: {\"Paris\": (\"GPE\", \"<GPE_0>\"), \"John Smith\": (\"PERSON\", \"<PERSON_1>\")}\n",
        "        \"\"\"\n",
        "        self.reset_counter()\n",
        "        entities = {}\n",
        "        \n",
        "        if self.method == \"spacy\" and self.nlp:\n",
        "            doc = self.nlp(text)\n",
        "            for ent in doc.ents:\n",
        "                if ent.text not in entities and len(ent.text.strip()) > 0:\n",
        "                    entity_type = ent.label_\n",
        "                    replacement = f\"<{entity_type}_{self.entity_counter}>\"\n",
        "                    entities[ent.text] = (entity_type, replacement)\n",
        "                    self.entity_counter += 1\n",
        "        else:\n",
        "            # Regex fallback\n",
        "            for entity_type, pattern in self._regex_patterns.items():\n",
        "                for match in pattern.finditer(text):\n",
        "                    entity_text = match.group()\n",
        "                    if entity_text not in entities and len(entity_text.strip()) > 0:\n",
        "                        replacement = f\"<{entity_type}_{self.entity_counter}>\"\n",
        "                        entities[entity_text] = (entity_type, replacement)\n",
        "                        self.entity_counter += 1\n",
        "        \n",
        "        return entities\n",
        "\n",
        "# Function words to preserve during masking\n",
        "FUNCTION_WORDS = {\n",
        "    # Articles\n",
        "    \"a\", \"an\", \"the\",\n",
        "    \n",
        "    # Prepositions\n",
        "    \"in\", \"on\", \"at\", \"by\", \"for\", \"with\", \"without\", \"to\", \"from\", \"of\", \"about\", \"under\", \"over\", \"through\",\n",
        "    \n",
        "    # Conjunctions\n",
        "    \"and\", \"or\", \"but\", \"so\", \"yet\", \"nor\", \"for\", \"because\", \"if\", \"when\", \"where\", \"while\", \"although\",\n",
        "    \n",
        "    # Pronouns\n",
        "    \"i\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\", \"me\", \"him\", \"her\", \"us\", \"them\", \"my\", \"your\", \"his\", \"her\", \"its\", \"our\", \"their\",\n",
        "    \"this\", \"that\", \"these\", \"those\", \"who\", \"whom\", \"whose\", \"which\", \"what\",\n",
        "    \n",
        "    # Auxiliary verbs\n",
        "    \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\",\n",
        "    \"will\", \"would\", \"shall\", \"should\", \"can\", \"could\", \"may\", \"might\", \"must\",\n",
        "    \n",
        "    # Common adverbs\n",
        "    \"not\", \"no\", \"yes\", \"very\", \"quite\", \"rather\", \"too\", \"so\", \"just\", \"only\", \"also\", \"even\", \"still\", \"already\", \"yet\",\n",
        "    \n",
        "    # Determiners\n",
        "    \"all\", \"some\", \"any\", \"many\", \"much\", \"few\", \"little\", \"more\", \"most\", \"other\", \"another\", \"each\", \"every\", \"both\", \"either\", \"neither\",\n",
        "}\n",
        "\n",
        "def is_function_word(word: str) -> bool:\n",
        "    \"\"\"Check if word is a function word.\"\"\"\n",
        "    return word.lower() in FUNCTION_WORDS\n",
        "\n",
        "def is_punctuation(token: str) -> bool:\n",
        "    \"\"\"Check if token is punctuation.\"\"\"\n",
        "    return bool(re.match(r'^[^\\w\\s]+$', token))\n",
        "\n",
        "print(\"âœ… NER Extractor and function word utilities loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EnhancedTokenizer:\n",
        "    \"\"\"\n",
        "    Enhanced tokenizer that handles structural hints and masking.\n",
        "    \n",
        "    Features:\n",
        "    - Base tokenizer (GPT-2) with extensions\n",
        "    - Special tokens for masking and entity types\n",
        "    - Entity ID management per document\n",
        "    - Structural hint preservation\n",
        "    - Îµ-masking applied BEFORE tokenization\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, base_model: str = \"gpt2\"):\n",
        "        \"\"\"\n",
        "        Initialize enhanced tokenizer.\n",
        "        \n",
        "        Args:\n",
        "            base_model: Base Hugging Face model name\n",
        "        \"\"\"\n",
        "        self.base_model = base_model\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "        \n",
        "        # Add special tokens\n",
        "        self.special_tokens = {\n",
        "            \"mask_token\": \"<mask>\",\n",
        "            \"unk_token\": \"<unk>\",\n",
        "            \"pad_token\": \"<pad>\",\n",
        "            \"bos_token\": \"<|startoftext|>\",\n",
        "            \"eos_token\": \"<|endoftext|>\",\n",
        "        }\n",
        "        \n",
        "        # Add special tokens to vocabulary\n",
        "        self.tokenizer.add_special_tokens(self.special_tokens)\n",
        "        \n",
        "        # Entity mapping for current document\n",
        "        self.entity_mapping = {}\n",
        "        self.entity_counter = 0\n",
        "        \n",
        "        # NER extractor\n",
        "        self.ner_extractor = NERExtractor()\n",
        "        \n",
        "        # Îµ-masker for pre-tokenization masking\n",
        "        self.epsilon_masker = EpsilonMasker()\n",
        "        \n",
        "        print(f\"âœ… Enhanced Tokenizer initialized with {base_model}\")\n",
        "        print(f\"   Vocabulary size: {self.tokenizer.vocab_size}\")\n",
        "        print(f\"   Special tokens: {list(self.special_tokens.keys())}\")\n",
        "    \n",
        "    def reset_entity_mapping(self):\n",
        "        \"\"\"Reset entity mapping for new document.\"\"\"\n",
        "        self.entity_mapping = {}\n",
        "        self.entity_counter = 0\n",
        "    \n",
        "    def create_entity_token(self, entity_type: str) -> str:\n",
        "        \"\"\"\n",
        "        Create a unique entity token for current document.\n",
        "        \n",
        "        Args:\n",
        "            entity_type: Type of entity (PERSON, ORG, LOC, etc.)\n",
        "            \n",
        "        Returns:\n",
        "            Unique entity token\n",
        "        \"\"\"\n",
        "        token = f\"<{entity_type}_{self.entity_counter}>\"\n",
        "        self.entity_counter += 1\n",
        "        return token\n",
        "    \n",
        "    def add_entity_tokens_to_vocab(self, entity_types: List[str]):\n",
        "        \"\"\"\n",
        "        Add entity type tokens to vocabulary.\n",
        "        \n",
        "        Args:\n",
        "            entity_types: List of entity types to add\n",
        "        \"\"\"\n",
        "        entity_tokens = {}\n",
        "        for entity_type in entity_types:\n",
        "            # Add a few numbered entity tokens\n",
        "            for i in range(10):  # Add tokens for 0-9\n",
        "                token = f\"<{entity_type}_{i}>\"\n",
        "                entity_tokens[token] = token\n",
        "        \n",
        "        self.tokenizer.add_tokens(list(entity_tokens.keys()))\n",
        "        print(f\"âœ… Added {len(entity_tokens)} entity tokens to vocabulary\")\n",
        "    \n",
        "    def tokenize_with_structural_hints(self, text: str, epsilon: float = 0.0, \n",
        "                                     max_length: int = 512, seed: Optional[int] = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Tokenize text with structural hints and masking.\n",
        "        \n",
        "        IMPORTANT: Îµ-masking is applied BEFORE tokenization using GPT-2 tokenizer.\n",
        "        \n",
        "        Args:\n",
        "            text: Input text\n",
        "            epsilon: Masking level (0.0 = no masking) - applied BEFORE tokenization\n",
        "            max_length: Maximum sequence length\n",
        "            seed: Random seed for reproducible masking\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary with tokenized input and metadata\n",
        "        \"\"\"\n",
        "        self.reset_entity_mapping()\n",
        "        \n",
        "        # STEP 1: Apply Îµ-masking BEFORE tokenization\n",
        "        if epsilon > 0.0:\n",
        "            masked_text, masking_stats = self.epsilon_masker.apply_masking(text, epsilon, seed)\n",
        "        else:\n",
        "            masked_text = text\n",
        "            masking_stats = self.epsilon_masker._get_empty_statistics()\n",
        "        \n",
        "        # STEP 2: Extract entities from masked text\n",
        "        entities = self.ner_extractor.extract_entities(masked_text)\n",
        "        \n",
        "        # STEP 3: Replace entities with typed tokens\n",
        "        final_text = masked_text\n",
        "        for entity_text, (entity_type, replacement) in entities.items():\n",
        "            final_text = final_text.replace(entity_text, replacement)\n",
        "        \n",
        "        # STEP 4: Tokenize with GPT-2 tokenizer\n",
        "        encoding = self.tokenizer(\n",
        "            final_text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        # STEP 5: Create comprehensive metadata\n",
        "        metadata = {\n",
        "            'original_text': text,\n",
        "            'masked_text': masked_text,\n",
        "            'final_text': final_text,\n",
        "            'epsilon': epsilon,\n",
        "            'entities_found': len(entities),\n",
        "            'entity_mapping': entities,\n",
        "            'mask_positions': self._get_mask_positions(encoding['input_ids'][0]),\n",
        "            'sequence_length': len(encoding['input_ids'][0]),\n",
        "            'masking_stats': masking_stats,\n",
        "            'tokenizer_used': self.base_model\n",
        "        }\n",
        "        \n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'][0],\n",
        "            'attention_mask': encoding['attention_mask'][0],\n",
        "            'metadata': metadata\n",
        "        }\n",
        "    \n",
        "    def _get_mask_positions(self, input_ids: torch.Tensor) -> List[int]:\n",
        "        \"\"\"Get positions of masked tokens.\"\"\"\n",
        "        mask_token_id = self.tokenizer.convert_tokens_to_ids(self.special_tokens['mask_token'])\n",
        "        mask_positions = []\n",
        "        \n",
        "        for i, token_id in enumerate(input_ids):\n",
        "            if token_id.item() == mask_token_id:\n",
        "                mask_positions.append(i)\n",
        "        \n",
        "        return mask_positions\n",
        "    \n",
        "    def decode_tokens(self, token_ids: Union[List[int], torch.Tensor]) -> str:\n",
        "        \"\"\"\n",
        "        Decode token IDs back to text.\n",
        "        \n",
        "        Args:\n",
        "            token_ids: List or tensor of token IDs\n",
        "            \n",
        "        Returns:\n",
        "            Decoded text\n",
        "        \"\"\"\n",
        "        if isinstance(token_ids, torch.Tensor):\n",
        "            token_ids = token_ids.tolist()\n",
        "        \n",
        "        return self.tokenizer.decode(token_ids, skip_special_tokens=False)\n",
        "    \n",
        "    def get_vocab_size(self) -> int:\n",
        "        \"\"\"Get current vocabulary size.\"\"\"\n",
        "        return len(self.tokenizer)\n",
        "    \n",
        "    def save_tokenizer(self, save_path: Union[str, Path]):\n",
        "        \"\"\"Save tokenizer configuration.\"\"\"\n",
        "        self.tokenizer.save_pretrained(save_path)\n",
        "        print(f\"âœ… Tokenizer saved to {save_path}\")\n",
        "    \n",
        "    def load_tokenizer(self, load_path: Union[str, Path]):\n",
        "        \"\"\"Load tokenizer configuration.\"\"\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(load_path)\n",
        "        print(f\"âœ… Tokenizer loaded from {load_path}\")\n",
        "\n",
        "# Initialize enhanced tokenizer\n",
        "ENHANCED_TOKENIZER = EnhancedTokenizer()\n",
        "print(\"âœ… Enhanced Tokenizer ready for use\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Îµ-Masking Implementation\n",
        "\n",
        "Core Îµ-masking functionality with structural hints preservation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EpsilonMasker:\n",
        "    \"\"\"\n",
        "    Applies Îµ-masking while preserving structural hints.\n",
        "    \n",
        "    Preserves:\n",
        "    - Function words (the, and, is, etc.)\n",
        "    - Punctuation marks\n",
        "    - NER entities (replaced with typed IDs)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, preserve_function_words: bool = True, \n",
        "                 preserve_punctuation: bool = True, \n",
        "                 preserve_ner: bool = True,\n",
        "                 ner_method: str = \"auto\"):\n",
        "        \"\"\"\n",
        "        Initialize Îµ-masker.\n",
        "        \n",
        "        Args:\n",
        "            preserve_function_words: Whether to preserve function words\n",
        "            preserve_punctuation: Whether to preserve punctuation\n",
        "            preserve_ner: Whether to preserve NER entities\n",
        "            ner_method: NER extraction method (\"spacy\", \"regex\", \"auto\")\n",
        "        \"\"\"\n",
        "        self.preserve_function_words = preserve_function_words\n",
        "        self.preserve_punctuation = preserve_punctuation\n",
        "        self.preserve_ner = preserve_ner\n",
        "        \n",
        "        # Initialize NER extractor\n",
        "        if preserve_ner:\n",
        "            self.ner_extractor = NERExtractor(method=ner_method)\n",
        "        else:\n",
        "            self.ner_extractor = None\n",
        "        \n",
        "        print(f\"âœ… Îµ-Masker initialized:\")\n",
        "        print(f\"   - Function words: {'âœ…' if preserve_function_words else 'âŒ'}\")\n",
        "        print(f\"   - Punctuation: {'âœ…' if preserve_punctuation else 'âŒ'}\")\n",
        "        print(f\"   - NER entities: {'âœ…' if preserve_ner else 'âŒ'}\")\n",
        "    \n",
        "    def apply_masking(self, text: str, epsilon: float, seed: Optional[int] = None) -> Tuple[str, Dict]:\n",
        "        \"\"\"\n",
        "        Apply Îµ-masking to text while preserving structural hints.\n",
        "        \n",
        "        Args:\n",
        "            text: Input text\n",
        "            epsilon: Masking probability (0.0-1.0)\n",
        "            seed: Random seed for reproducibility\n",
        "            \n",
        "        Returns:\n",
        "            Tuple of (masked_text, statistics)\n",
        "        \"\"\"\n",
        "        if seed is not None:\n",
        "            random.seed(seed)\n",
        "            np.random.seed(seed)\n",
        "        \n",
        "        if epsilon == 0.0:\n",
        "            return text, self._get_empty_statistics()\n",
        "        \n",
        "        # Extract entities if NER is enabled\n",
        "        entities = {}\n",
        "        if self.preserve_ner and self.ner_extractor:\n",
        "            entities = self.ner_extractor.extract_entities(text)\n",
        "        \n",
        "        # Apply masking\n",
        "        masked_text = self._mask_text(text, epsilon, entities)\n",
        "        \n",
        "        # Get statistics\n",
        "        stats = self.get_masking_statistics(text, masked_text, epsilon, entities)\n",
        "        \n",
        "        return masked_text, stats\n",
        "    \n",
        "    def _mask_text(self, text: str, epsilon: float, entities: Dict) -> str:\n",
        "        \"\"\"Apply masking to text.\"\"\"\n",
        "        masked_text = text\n",
        "        \n",
        "        # Replace entities with typed tokens\n",
        "        if entities:\n",
        "            for entity_text, (entity_type, replacement) in entities.items():\n",
        "                masked_text = masked_text.replace(entity_text, replacement)\n",
        "        \n",
        "        # Tokenize into words for masking\n",
        "        words = re.findall(r'\\S+', masked_text)\n",
        "        masked_words = []\n",
        "        \n",
        "        for word in words:\n",
        "            # Check if word should be preserved\n",
        "            if self._should_preserve_word(word):\n",
        "                masked_words.append(word)\n",
        "            else:\n",
        "                # Apply masking with probability epsilon\n",
        "                if random.random() < epsilon:\n",
        "                    masked_words.append(\"<mask>\")\n",
        "                else:\n",
        "                    masked_words.append(word)\n",
        "        \n",
        "        return ' '.join(masked_words)\n",
        "    \n",
        "    def _should_preserve_word(self, word: str) -> bool:\n",
        "        \"\"\"Check if word should be preserved during masking.\"\"\"\n",
        "        # Preserve entity tokens\n",
        "        if word.startswith('<') and word.endswith('>'):\n",
        "            return True\n",
        "        \n",
        "        # Preserve function words\n",
        "        if self.preserve_function_words and is_function_word(word):\n",
        "            return True\n",
        "        \n",
        "        # Preserve punctuation\n",
        "        if self.preserve_punctuation and is_punctuation(word):\n",
        "            return True\n",
        "        \n",
        "        return False\n",
        "    \n",
        "    def get_masking_statistics(self, original_text: str, masked_text: str, \n",
        "                             epsilon: float, entities: Dict) -> Dict:\n",
        "        \"\"\"\n",
        "        Get statistics about masking applied to text.\n",
        "        \n",
        "        Args:\n",
        "            original_text: Original text\n",
        "            masked_text: Masked text\n",
        "            epsilon: Masking level\n",
        "            entities: Extracted entities\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary with masking statistics\n",
        "        \"\"\"\n",
        "        original_words = re.findall(r'\\S+', original_text)\n",
        "        masked_words = re.findall(r'\\S+', masked_text)\n",
        "        \n",
        "        # Count different types of tokens\n",
        "        total_tokens = len(original_words)\n",
        "        masked_tokens = masked_words.count('<mask>')\n",
        "        entity_tokens = sum(1 for word in masked_words if word.startswith('<') and word.endswith('>'))\n",
        "        function_word_tokens = sum(1 for word in masked_words if is_function_word(word))\n",
        "        punctuation_tokens = sum(1 for word in masked_words if is_punctuation(word))\n",
        "        \n",
        "        return {\n",
        "            'total_tokens': total_tokens,\n",
        "            'masked_tokens': masked_tokens,\n",
        "            'entity_tokens': entity_tokens,\n",
        "            'function_word_tokens': function_word_tokens,\n",
        "            'punctuation_tokens': punctuation_tokens,\n",
        "            'preserved_tokens': entity_tokens + function_word_tokens + punctuation_tokens,\n",
        "            'masking_rate': masked_tokens / total_tokens if total_tokens > 0 else 0,\n",
        "            'preservation_rate': (entity_tokens + function_word_tokens + punctuation_tokens) / total_tokens if total_tokens > 0 else 0,\n",
        "            'epsilon': epsilon,\n",
        "            'entities_found': len(entities),\n",
        "            'entity_types': list(set(entity_type for _, (entity_type, _) in entities.items()))\n",
        "        }\n",
        "    \n",
        "    def _get_empty_statistics(self) -> Dict:\n",
        "        \"\"\"Get empty statistics for epsilon=0.\"\"\"\n",
        "        return {\n",
        "            'total_tokens': 0,\n",
        "            'masked_tokens': 0,\n",
        "            'entity_tokens': 0,\n",
        "            'function_word_tokens': 0,\n",
        "            'punctuation_tokens': 0,\n",
        "            'preserved_tokens': 0,\n",
        "            'masking_rate': 0.0,\n",
        "            'preservation_rate': 0.0,\n",
        "            'epsilon': 0.0,\n",
        "            'entities_found': 0,\n",
        "            'entity_types': []\n",
        "        }\n",
        "    \n",
        "    def set_random_seed(self, seed: int):\n",
        "        \"\"\"Set random seed for reproducible masking.\"\"\"\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        print(f\"âœ… Random seed set to {seed}\")\n",
        "\n",
        "# Initialize Îµ-masker\n",
        "EPSILON_MASKER = EpsilonMasker()\n",
        "print(\"âœ… Îµ-Masker ready for use\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualization and Analysis Tools\n",
        "\n",
        "Tools for analyzing and visualizing tokenization results and masking effects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "class TokenizationAnalyzer:\n",
        "    \"\"\"\n",
        "    Analysis and visualization tools for tokenization results.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.results_cache = []\n",
        "    \n",
        "    def analyze_text(self, text: str, epsilon_values: List[float] = [0.0, 0.1, 0.3, 0.5, 0.7]) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Analyze text with different epsilon values.\n",
        "        \n",
        "        Args:\n",
        "            text: Input text to analyze\n",
        "            epsilon_values: List of epsilon values to test\n",
        "            \n",
        "        Returns:\n",
        "            DataFrame with analysis results\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        \n",
        "        for epsilon in epsilon_values:\n",
        "            # Apply masking\n",
        "            masked_text, stats = EPSILON_MASKER.apply_masking(text, epsilon, seed=42)\n",
        "            \n",
        "            # Tokenize with enhanced tokenizer\n",
        "            tokenized = ENHANCED_TOKENIZER.tokenize_with_structural_hints(masked_text, epsilon=0.0)\n",
        "            \n",
        "            # Combine results\n",
        "            result = {\n",
        "                'epsilon': epsilon,\n",
        "                'original_text': text,\n",
        "                'masked_text': masked_text,\n",
        "                'tokenized_text': ENHANCED_TOKENIZER.decode_tokens(tokenized['input_ids']),\n",
        "                'sequence_length': tokenized['metadata']['sequence_length'],\n",
        "                'mask_positions': len(tokenized['metadata']['mask_positions']),\n",
        "                **stats\n",
        "            }\n",
        "            results.append(result)\n",
        "        \n",
        "        df = pd.DataFrame(results)\n",
        "        self.results_cache = df\n",
        "        return df\n",
        "    \n",
        "    def plot_masking_effects(self, df: Optional[pd.DataFrame] = None):\n",
        "        \"\"\"\n",
        "        Plot masking effects across different epsilon values.\n",
        "        \n",
        "        Args:\n",
        "            df: Analysis results DataFrame (uses cache if None)\n",
        "        \"\"\"\n",
        "        if df is None:\n",
        "            df = self.results_cache\n",
        "        \n",
        "        if df.empty:\n",
        "            print(\"No analysis results available. Run analyze_text() first.\")\n",
        "            return\n",
        "        \n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        fig.suptitle('Îµ-Masking Effects Analysis', fontsize=16)\n",
        "        \n",
        "        # Plot 1: Masking rate vs epsilon\n",
        "        axes[0, 0].plot(df['epsilon'], df['masking_rate'], 'bo-', linewidth=2, markersize=8)\n",
        "        axes[0, 0].set_xlabel('Epsilon (Îµ)')\n",
        "        axes[0, 0].set_ylabel('Masking Rate')\n",
        "        axes[0, 0].set_title('Masking Rate vs Epsilon')\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Plot 2: Preservation rate vs epsilon\n",
        "        axes[0, 1].plot(df['epsilon'], df['preservation_rate'], 'go-', linewidth=2, markersize=8)\n",
        "        axes[0, 1].set_xlabel('Epsilon (Îµ)')\n",
        "        axes[0, 1].set_ylabel('Preservation Rate')\n",
        "        axes[0, 1].set_title('Structural Hints Preservation')\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Plot 3: Token type distribution\n",
        "        token_types = ['masked_tokens', 'entity_tokens', 'function_word_tokens', 'punctuation_tokens']\n",
        "        x = np.arange(len(df))\n",
        "        width = 0.2\n",
        "        \n",
        "        for i, token_type in enumerate(token_types):\n",
        "            axes[1, 0].bar(x + i*width, df[token_type], width, label=token_type.replace('_tokens', ''))\n",
        "        \n",
        "        axes[1, 0].set_xlabel('Epsilon Values')\n",
        "        axes[1, 0].set_ylabel('Token Count')\n",
        "        axes[1, 0].set_title('Token Type Distribution')\n",
        "        axes[1, 0].set_xticks(x + width * 1.5)\n",
        "        axes[1, 0].set_xticklabels(df['epsilon'])\n",
        "        axes[1, 0].legend()\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Plot 4: Entities found\n",
        "        axes[1, 1].bar(df['epsilon'], df['entities_found'], color='purple', alpha=0.7)\n",
        "        axes[1, 1].set_xlabel('Epsilon (Îµ)')\n",
        "        axes[1, 1].set_ylabel('Entities Found')\n",
        "        axes[1, 1].set_title('Named Entities Preserved')\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    def compare_texts(self, texts: List[str], epsilon: float = 0.3):\n",
        "        \"\"\"\n",
        "        Compare masking effects across multiple texts.\n",
        "        \n",
        "        Args:\n",
        "            texts: List of texts to compare\n",
        "            epsilon: Epsilon value to use\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        \n",
        "        for i, text in enumerate(texts):\n",
        "            masked_text, stats = EPSILON_MASKER.apply_masking(text, epsilon, seed=42)\n",
        "            results.append({\n",
        "                'text_id': i,\n",
        "                'original': text,\n",
        "                'masked': masked_text,\n",
        "                'masking_rate': stats['masking_rate'],\n",
        "                'preservation_rate': stats['preservation_rate'],\n",
        "                'entities_found': stats['entities_found']\n",
        "            })\n",
        "        \n",
        "        df = pd.DataFrame(results)\n",
        "        \n",
        "        # Create comparison plot\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "        \n",
        "        # Masking rate comparison\n",
        "        axes[0].bar(range(len(df)), df['masking_rate'], color='red', alpha=0.7)\n",
        "        axes[0].set_xlabel('Text ID')\n",
        "        axes[0].set_ylabel('Masking Rate')\n",
        "        axes[0].set_title(f'Masking Rate Comparison (Îµ={epsilon})')\n",
        "        axes[0].set_xticks(range(len(df)))\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Preservation rate comparison\n",
        "        axes[1].bar(range(len(df)), df['preservation_rate'], color='green', alpha=0.7)\n",
        "        axes[1].set_xlabel('Text ID')\n",
        "        axes[1].set_ylabel('Preservation Rate')\n",
        "        axes[1].set_title(f'Preservation Rate Comparison (Îµ={epsilon})')\n",
        "        axes[1].set_xticks(range(len(df)))\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Entities comparison\n",
        "        axes[2].bar(range(len(df)), df['entities_found'], color='purple', alpha=0.7)\n",
        "        axes[2].set_xlabel('Text ID')\n",
        "        axes[2].set_ylabel('Entities Found')\n",
        "        axes[2].set_title(f'Named Entities Comparison (Îµ={epsilon})')\n",
        "        axes[2].set_xticks(range(len(df)))\n",
        "        axes[2].grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def generate_report(self, df: Optional[pd.DataFrame] = None) -> str:\n",
        "        \"\"\"\n",
        "        Generate a text report of analysis results.\n",
        "        \n",
        "        Args:\n",
        "            df: Analysis results DataFrame (uses cache if None)\n",
        "            \n",
        "        Returns:\n",
        "            Formatted report string\n",
        "        \"\"\"\n",
        "        if df is None:\n",
        "            df = self.results_cache\n",
        "        \n",
        "        if df.empty:\n",
        "            return \"No analysis results available.\"\n",
        "        \n",
        "        report = []\n",
        "        report.append(\"=\" * 60)\n",
        "        report.append(\"TOKENIZATION ANALYSIS REPORT\")\n",
        "        report.append(\"=\" * 60)\n",
        "        \n",
        "        report.append(f\"\\nAnalyzed {len(df)} different epsilon values:\")\n",
        "        report.append(f\"Epsilon range: {df['epsilon'].min():.1f} - {df['epsilon'].max():.1f}\")\n",
        "        \n",
        "        report.append(f\"\\nMASKING STATISTICS:\")\n",
        "        report.append(f\"  Average masking rate: {df['masking_rate'].mean():.3f}\")\n",
        "        report.append(f\"  Average preservation rate: {df['preservation_rate'].mean():.3f}\")\n",
        "        report.append(f\"  Total entities found: {df['entities_found'].sum()}\")\n",
        "        \n",
        "        report.append(f\"\\nTOKEN BREAKDOWN:\")\n",
        "        report.append(f\"  Total tokens analyzed: {df['total_tokens'].sum()}\")\n",
        "        report.append(f\"  Masked tokens: {df['masked_tokens'].sum()}\")\n",
        "        report.append(f\"  Entity tokens: {df['entity_tokens'].sum()}\")\n",
        "        report.append(f\"  Function word tokens: {df['function_word_tokens'].sum()}\")\n",
        "        report.append(f\"  Punctuation tokens: {df['punctuation_tokens'].sum()}\")\n",
        "        \n",
        "        report.append(f\"\\nENTITY TYPES FOUND:\")\n",
        "        all_entity_types = set()\n",
        "        for entity_types in df['entity_types']:\n",
        "            all_entity_types.update(entity_types)\n",
        "        report.append(f\"  {', '.join(sorted(all_entity_types))}\")\n",
        "        \n",
        "        report.append(\"\\n\" + \"=\" * 60)\n",
        "        \n",
        "        return \"\\n\".join(report)\n",
        "\n",
        "# Initialize analyzer\n",
        "ANALYZER = TokenizationAnalyzer()\n",
        "print(\"âœ… Tokenization Analyzer ready for use\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Testing Examples and Demonstrations\n",
        "\n",
        "Comprehensive examples demonstrating the tokenization pipeline with Îµ-masking.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Example 1: Basic Îµ-masking demonstration\n",
        "print(\"=\" * 80)\n",
        "print(\"EXAMPLE 1: Basic Îµ-masking with GPT-2 tokenization\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "sample_text = \"The quick brown fox jumps over the lazy dog in New York City.\"\n",
        "\n",
        "print(f\"Original text: {sample_text}\")\n",
        "print()\n",
        "\n",
        "# Test different epsilon values\n",
        "epsilon_values = [0.0, 0.2, 0.5, 0.8]\n",
        "\n",
        "for epsilon in epsilon_values:\n",
        "    print(f\"Îµ = {epsilon}:\")\n",
        "    \n",
        "    # Apply masking and tokenization\n",
        "    result = ENHANCED_TOKENIZER.tokenize_with_structural_hints(\n",
        "        sample_text, \n",
        "        epsilon=epsilon, \n",
        "        seed=42\n",
        "    )\n",
        "    \n",
        "    print(f\"  Masked text: {result['metadata']['masked_text']}\")\n",
        "    print(f\"  Final text: {result['metadata']['final_text']}\")\n",
        "    print(f\"  Tokens: {len(result['input_ids'])}\")\n",
        "    print(f\"  Mask positions: {len(result['metadata']['mask_positions'])}\")\n",
        "    print(f\"  Entities found: {result['metadata']['entities_found']}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Example 2: GCS Data Loading (if available)\n",
        "print(\"=\" * 80)\n",
        "print(\"EXAMPLE 2: GCS Data Loading and Processing\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if GCS_LOADER is not None:\n",
        "    try:\n",
        "        # List files in bucket\n",
        "        files = GCS_LOADER.list_files()\n",
        "        print(f\"Found {len(files)} files in bucket\")\n",
        "        \n",
        "        if files:\n",
        "            # Get file info\n",
        "            info = GCS_LOADER.get_file_info(files[:5])  # Sample first 5 files\n",
        "            print(f\"Sample file info:\")\n",
        "            print(f\"  Total files: {info['total_files']}\")\n",
        "            print(f\"  Sample files: {info['sample_files']}\")\n",
        "            print(f\"  Average file size: {info.get('avg_file_size_mb', 0):.2f} MB\")\n",
        "            \n",
        "            # Load a small sample\n",
        "            print(f\"\\nLoading sample data...\")\n",
        "            sample_df = GCS_LOADER.load_batch_files(files[:2])  # Load first 2 files\n",
        "            \n",
        "            if not sample_df.empty:\n",
        "                print(f\"Loaded {len(sample_df)} rows\")\n",
        "                print(f\"Columns: {list(sample_df.columns)}\")\n",
        "                \n",
        "                # Process a few samples\n",
        "                if 'text' in sample_df.columns:\n",
        "                    sample_texts = sample_df['text'].head(3).tolist()\n",
        "                    print(f\"\\nProcessing sample texts with Îµ-masking:\")\n",
        "                    \n",
        "                    for i, text in enumerate(sample_texts):\n",
        "                        if isinstance(text, str) and len(text) > 50:\n",
        "                            print(f\"\\nSample {i+1}:\")\n",
        "                            print(f\"Original: {text[:100]}...\")\n",
        "                            \n",
        "                            # Apply masking\n",
        "                            masked_text, stats = EPSILON_MASKER.apply_masking(text, 0.3, seed=42)\n",
        "                            print(f\"Masked: {masked_text[:100]}...\")\n",
        "                            print(f\"Masking rate: {stats['masking_rate']:.3f}\")\n",
        "                            print(f\"Preservation rate: {stats['preservation_rate']:.3f}\")\n",
        "                            print(f\"Entities found: {stats['entities_found']}\")\n",
        "            else:\n",
        "                print(\"No data loaded\")\n",
        "        else:\n",
        "            print(\"No files found in bucket\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing GCS: {e}\")\n",
        "else:\n",
        "    print(\"GCS not available - skipping GCS examples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Example 3: Comprehensive Analysis with Visualization\n",
        "print(\"=\" * 80)\n",
        "print(\"EXAMPLE 3: Comprehensive Analysis with Visualization\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Test texts with different characteristics\n",
        "test_texts = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"John Smith works at Microsoft Corporation in Seattle, Washington.\",\n",
        "    \"The meeting is scheduled for January 15, 2024 at 3:00 PM.\",\n",
        "    \"She bought a car for $25,000 from Toyota Motors Inc.\"\n",
        "]\n",
        "\n",
        "print(\"Analyzing texts with different epsilon values...\")\n",
        "\n",
        "# Analyze each text\n",
        "for i, text in enumerate(test_texts):\n",
        "    print(f\"\\nText {i+1}: {text}\")\n",
        "    \n",
        "    # Run analysis\n",
        "    df = ANALYZER.analyze_text(text, epsilon_values=[0.0, 0.1, 0.3, 0.5, 0.7])\n",
        "    \n",
        "    # Show results\n",
        "    print(\"Results:\")\n",
        "    for _, row in df.iterrows():\n",
        "        print(f\"  Îµ={row['epsilon']}: masking_rate={row['masking_rate']:.3f}, \"\n",
        "              f\"preservation_rate={row['preservation_rate']:.3f}, \"\n",
        "              f\"entities={row['entities_found']}\")\n",
        "\n",
        "print(f\"\\nGenerating visualization...\")\n",
        "# Create visualization for the first text\n",
        "df = ANALYZER.analyze_text(test_texts[0], epsilon_values=[0.0, 0.1, 0.3, 0.5, 0.7])\n",
        "ANALYZER.plot_masking_effects(df)\n",
        "\n",
        "print(f\"\\nGenerating comparison across texts...\")\n",
        "# Compare texts\n",
        "comparison_df = ANALYZER.compare_texts(test_texts, epsilon=0.3)\n",
        "\n",
        "print(f\"\\nAnalysis Report:\")\n",
        "report = ANALYZER.generate_report(df)\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 4: Performance Benchmarking\n",
        "print(\"=\" * 80)\n",
        "print(\"EXAMPLE 4: Performance Benchmarking\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "import time\n",
        "\n",
        "def benchmark_tokenization(text: str, epsilon_values: List[float], iterations: int = 10):\n",
        "    \"\"\"Benchmark tokenization performance across different epsilon values.\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for epsilon in epsilon_values:\n",
        "        times = []\n",
        "        \n",
        "        for _ in range(iterations):\n",
        "            start_time = time.time()\n",
        "            \n",
        "            # Apply masking and tokenization\n",
        "            result = ENHANCED_TOKENIZER.tokenize_with_structural_hints(\n",
        "                text, \n",
        "                epsilon=epsilon, \n",
        "                seed=42\n",
        "            )\n",
        "            \n",
        "            end_time = time.time()\n",
        "            times.append(end_time - start_time)\n",
        "        \n",
        "        avg_time = np.mean(times)\n",
        "        std_time = np.std(times)\n",
        "        \n",
        "        results.append({\n",
        "            'epsilon': epsilon,\n",
        "            'avg_time_ms': avg_time * 1000,\n",
        "            'std_time_ms': std_time * 1000,\n",
        "            'tokens_per_sec': len(result['input_ids']) / avg_time,\n",
        "            'sequence_length': len(result['input_ids'])\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Benchmark with a longer text\n",
        "long_text = \"\"\"\n",
        "The artificial intelligence revolution has transformed numerous industries and continues to shape our daily lives. \n",
        "Machine learning algorithms, powered by vast datasets and computational resources, have achieved remarkable breakthroughs \n",
        "in natural language processing, computer vision, and autonomous systems. Companies like Google, Microsoft, and OpenAI \n",
        "have invested billions of dollars in developing advanced AI models that can understand, generate, and manipulate human language \n",
        "with unprecedented sophistication. These developments raise important questions about the future of work, privacy, and \n",
        "the ethical implications of increasingly powerful AI systems.\n",
        "\"\"\"\n",
        "\n",
        "print(f\"Benchmarking tokenization performance...\")\n",
        "print(f\"Text length: {len(long_text)} characters\")\n",
        "\n",
        "# Run benchmark\n",
        "benchmark_df = benchmark_tokenization(long_text, [0.0, 0.1, 0.3, 0.5, 0.7], iterations=5)\n",
        "\n",
        "print(f\"\\nBenchmark Results:\")\n",
        "print(benchmark_df.to_string(index=False))\n",
        "\n",
        "# Create performance visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot 1: Processing time vs epsilon\n",
        "axes[0].errorbar(benchmark_df['epsilon'], benchmark_df['avg_time_ms'], \n",
        "                 yerr=benchmark_df['std_time_ms'], marker='o', capsize=5)\n",
        "axes[0].set_xlabel('Epsilon (Îµ)')\n",
        "axes[0].set_ylabel('Processing Time (ms)')\n",
        "axes[0].set_title('Tokenization Performance vs Epsilon')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Tokens per second vs epsilon\n",
        "axes[1].plot(benchmark_df['epsilon'], benchmark_df['tokens_per_sec'], 'go-', linewidth=2, markersize=8)\n",
        "axes[1].set_xlabel('Epsilon (Îµ)')\n",
        "axes[1].set_ylabel('Tokens per Second')\n",
        "axes[1].set_title('Throughput vs Epsilon')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nPerformance Summary:\")\n",
        "print(f\"  Fastest processing: Îµ={benchmark_df.loc[benchmark_df['avg_time_ms'].idxmin(), 'epsilon']:.1f}\")\n",
        "print(f\"  Slowest processing: Îµ={benchmark_df.loc[benchmark_df['avg_time_ms'].idxmax(), 'epsilon']:.1f}\")\n",
        "print(f\"  Average tokens/sec: {benchmark_df['tokens_per_sec'].mean():.1f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "deep",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
